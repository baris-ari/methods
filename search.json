[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Science Research Methods",
    "section": "",
    "text": "First Task\nOur very first task is to install R and R Studio on our laptops.\nPlease try to do this before coming to next lab.\nR and R Studio are very powerful tools for analysing data and for creating high-quality documents. I prepared this website using R Studio. It is widely used both in academic research and in commercial enterprise. Learning the fundamentals of these powerful tools gives you an advantage in the job market (or for pursuing further studies such as PhD). They are free and open source.\nMake sure to install R first and then the R Studio.\nInstructions for installing R and R Studio are available in Appendix A of the Online Textbook Hands-on Programming with R.",
    "crumbs": [
      "Welcome to Lab Website"
    ]
  },
  {
    "objectID": "index.html#first-task",
    "href": "index.html#first-task",
    "title": "Social Science Research Methods",
    "section": "",
    "text": "R can be installed here: https://cran.r-project.org/\nR Studio can be installed here: https://posit.co/downloads/",
    "crumbs": [
      "Welcome to Lab Website"
    ]
  },
  {
    "objectID": "01_rbasics.html",
    "href": "01_rbasics.html",
    "title": "1  R Basics",
    "section": "",
    "text": "1.1 Objectives for Week 1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "01_rbasics.html#objectives-for-week-1",
    "href": "01_rbasics.html#objectives-for-week-1",
    "title": "1  R Basics",
    "section": "",
    "text": "Use R as a calculator\nWrite and execute a command by using R Studio text editor\nSave your script\nUse the assignment operator to create objects\nUnderstand the difference between ‘string’ and ‘numerical’\nCreate a simple dataset",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "01_rbasics.html#use-r-as-a-calculator",
    "href": "01_rbasics.html#use-r-as-a-calculator",
    "title": "1  R Basics",
    "section": "1.2 Use R as a calculator",
    "text": "1.2 Use R as a calculator\nGo to the console pane and type a simple calculation.\n\n1 + 3\n\n[1] 4\n\n\n\n\n\n\n\n\nFigure 1.2: Our first calculation\n\n\n\nAs you can see, the output for 1 + 3 is 4, which is correct. We directly did a calculation using the console.\nThis would work, but it is not a good approach. Do not write your code directly to the console. Instead, go to the top left pane and write your ‘code’ into the text editor. The calculation 1 + 3 here is your code.\n\nSave your script by File &gt;&gt; Save OR simply by pressing . It is a good idea to create a folder/directory for this module and give your script an intuitive name such as learn_01.R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "01_rbasics.html#assignment-operator-to-create-objects",
    "href": "01_rbasics.html#assignment-operator-to-create-objects",
    "title": "1  R Basics",
    "section": "1.3 Assignment operator to create objects",
    "text": "1.3 Assignment operator to create objects\nWe can create objects in R which store our data. For example, you would like to calculate your age. Current year (i.e., 2024) - your birth year gives your age.\nLet’s create an object which stores your year of birth. We are going to call it my_birth_year. Each R object must be one-word only, so I use _ instead of space. We could also have used a dot or dash.\n\n# This is a comment. \n# Characters after a hashtag are considered as comments by R. \n# They are not executed.\n# Use comments extensively to take notes \n# and to remind your future self of the work you did. \n\n# \"&lt;-\" is the assignment operator\n# It basically symbolizes an arrow.\n\nmy_birth_year &lt;- 1985\n\nNow the Environment should store an object called my_birth_year. When I run my_birth_year, R will display the information stored.\n\nmy_birth_year\n\n[1] 1985\n\n\nNote that R is case sensitive. If you mistype, such as My_birth_year, it will give you an error message.\n\nMy_birth_year\n\nError: object 'My_birth_year' not found\n\n\nWe can find your age by subtracting current year from my_birth_year.\n\n2024 - my_birth_year\n\n[1] 39\n\n\nWe typed 2024 manually. We might want to create another object called current_year. Try to do it yourself, as an exercise.\n\n\nShow the code\ncurrent_year &lt;- 2024\n\n\nYou can do operations using objects. For example, calculate your age using the objects current_year and my_birth_year. Store this in another object called my_age.\n\nmy_age &lt;- current_year - my_birth_year \n\nCheck if you did correctly.\n\nmy_age\n\n[1] 39\n\n\nYou can also write over an object.\n\ncurrent_year &lt;- 2030\ncurrent_year\n\n[1] 2030\n\n\nThis would not change outputs previously created using the older version of the objects.\n\nmy_age\n\n[1] 39\n\n\nObviously, current year is not 2030, so let’s correct it back.\n\ncurrent_year &lt;- 2024",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "01_rbasics.html#numerical-and-string-objects",
    "href": "01_rbasics.html#numerical-and-string-objects",
    "title": "1  R Basics",
    "section": "1.4 Numerical and String Objects",
    "text": "1.4 Numerical and String Objects\nSo far, we stored numerical data. We can also have textual information, such as name of a person, or type of a medicine.\nCreate an object called my_name and store your name there.\n\nmy_name &lt;- \"Baris\"\nmy_name\n\n[1] \"Baris\"\n\n\nAs you can see, R displays textual information within quotation (““). Any information stored or displayed within ’’ is called a string and refers to text.\nCreate an object called my_name_last and store your name there.\n\nmy_last_name &lt;- \"Ari\"\n\nObviously, you cannot make a calculation using words. It is nonsensical to subtract two words. You cannot do any calculation with words.\n\nmy_name_last - my_name\n\nError: object 'my_name_last' not found\n\n\nSometimes numerical information is stored as text. In that case, R will not consider it as a number. For example, see three objects below.\n\nnum1 &lt;- 10\nnum2 &lt;- 100\nnum3 &lt;- \"1000\"\n\nnum1 and num2 are numerical values, but num3 is text. You cannot do any calculation with that.\n\nnum1\n\n[1] 10\n\nnum2\n\n[1] 100\n\nnum3\n\n[1] \"1000\"\n\nnum1 + num2\n\n[1] 110\n\nnum1 + num3\n\nError in num1 + num3: non-numeric argument to binary operator",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "01_rbasics.html#create-a-simple-dataset",
    "href": "01_rbasics.html#create-a-simple-dataset",
    "title": "1  R Basics",
    "section": "1.5 Create a simple dataset",
    "text": "1.5 Create a simple dataset\nImagine that we have the names and birth years of a number of people. We cannot really hold each piece of information in separate objects. We would like to store them altogether in a single object, like a spreadsheet.\nLet’s start with names. We have eight people:\n\nKeir Starmer\nRishi Sunak\nLiz Truss\nBoris Johnson\nTheresa May\nDavid Cameron\nGordon Brown\nTony Blair\n\nWe can store their full names in a single object using the combine function c().\n\nnames_pm &lt;- c(\"Keir Starmer\",\n              \"Rishi Sunak\",\n              \"Liz Truss\",\n              \"Boris Johnson\",\n              \"Theresa May\",\n              \"David Cameron\",\n              \"Gordon Brown\",\n              \"Tony Blair\")\n\nNote that each PMs name is written within quotation and they are combined together with the function c(). Each item within c() is separated with a comma. Let’s see the object:\n\nnames_pm\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n\nGreat! We have the names of the last six UK PMs.\nYou may have realized that there are numbers in squared brackets in the beginning of each line.\nThese numbers refer to the order in the sequence. For example, “Keir Starmer” is the first item whereas “Theresa May” is the fifth.\nYou can recall a particular item in the object using square brackets. Let’s print the first item in names_pm.\n\nnames_pm[1]\n\n[1] \"Keir Starmer\"\n\n\nSimilarly, for the third item, you would use [3]:\n\nnames_pm[3]\n\n[1] \"Liz Truss\"\n\n\nFind the fifth name in the object.\n\nnames_pm[5]\n\n[1] \"Theresa May\"\n\n\nYou can add more than one number into the square brackets using the c() function. For example, who are the second and fourth names?\n\nnames_pm[c(2,4)]\n\n[1] \"Rishi Sunak\"   \"Boris Johnson\"\n\n\nNext, let’s write down their birth year. The order is important! You need to keep the same order with PMs.\n\nbirth_years &lt;- c(1962, # Keir Starmer\n                 1980, # Rishi Sunak\n                 1975, # Liz Truss\n                 1964, # Boris Johnson\n                 1956, # Theresa May\n                 1966, # David Cameron\n                 1951, # Gordon Brown\n                 1953  # Tony Blair)\n                 )\n\nCheck the object we just created.\n\nbirth_years\n\n[1] 1962 1980 1975 1964 1956 1966 1951 1953\n\n\nLet’s put them together in a spreadsheet. What we would like to do is to vertically bind the two objects, which is called column bind and denoted with cbind().\n\ncbind(names_pm, birth_years)\n\n     names_pm        birth_years\n[1,] \"Keir Starmer\"  \"1962\"     \n[2,] \"Rishi Sunak\"   \"1980\"     \n[3,] \"Liz Truss\"     \"1975\"     \n[4,] \"Boris Johnson\" \"1964\"     \n[5,] \"Theresa May\"   \"1956\"     \n[6,] \"David Cameron\" \"1966\"     \n[7,] \"Gordon Brown\"  \"1951\"     \n[8,] \"Tony Blair\"    \"1953\"     \n\n\nSo far, we just printed this on our screen but we have not stored it in an object. Put this into an object.\n\nmy_data &lt;- cbind(names_pm, birth_years)\n\nCheck my_data.\n\nmy_data\n\n     names_pm        birth_years\n[1,] \"Keir Starmer\"  \"1962\"     \n[2,] \"Rishi Sunak\"   \"1980\"     \n[3,] \"Liz Truss\"     \"1975\"     \n[4,] \"Boris Johnson\" \"1964\"     \n[5,] \"Theresa May\"   \"1956\"     \n[6,] \"David Cameron\" \"1966\"     \n[7,] \"Gordon Brown\"  \"1951\"     \n[8,] \"Tony Blair\"    \"1953\"     \n\n\nNote that birth_years are stored as text, not numbers. I know this because they are within quotation marks.\nIt is customary to keep spreadsheets as something called “data frames” in R. This will not change our data, but makes further operations easier by unlocking some of the features of R.\n\nmy_data &lt;- as.data.frame(my_data)\n\nWe can take a better look at the dataset using View() function.\n\n# View(my_data)\n\nLet’s save our script.\n\n1.5.1 Variables in a data frame\nColumns in a data frame are also called variables. We have two variables in the dataset:\n\nnames_pm : Name of the UK PM\nbirth_years: Birth year of the PM\n\nThere are a few ways to access a variable. A straightforward approach is to use the $ notation:\n\n# 'name of the data frame'$'name of the variable' \nmy_data$names_pm\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n\nNow it is your turn. Display the birth_years variable.\n\nmy_data$birth_years\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n\nYou can think this expression as a sentence in R. In plain English, this expression tells R to bring the variable names_pm within the data frame my_data. The symbol $ refers to the ‘within’ part of this sentence.\nJust like you can convey the same meaning using different sentence structures, there are different ways to do the same thing in R. This is because R is working exactly like a language: it is a language to communicate with the computer.\nAnother way is using the square brackets notation []. names_pm is the first column in the data frame. To get the variable, you could type the following:\n\nmy_data[,1]\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n\nNote that we did not simply write my_data[1]. There is a comma: my_data[,1]\nIn a spreadsheet, we have two dimensions: rows and columns. By convention, rows are considered as the first dimension, and columns are considered as the second. This is why we had to use a comma to designate that we are interested in columns. If left the first dimension unspecified, which tells R to bring everything.\nIf you want to get the first row, you would type the following:\n\nmy_data[1, ]\n\n      names_pm birth_years\n1 Keir Starmer        1962\n\n\nTry it yourself; get the fourth row.\n\n\nShow the code\nmy_data[4,]\n\n\n       names_pm birth_years\n4 Boris Johnson        1964\n\n\nLet’s put these together: you can tell R to bring a specific observation. For example, third row of second column.\n\nmy_data[3,2]\n\n[1] \"1975\"\n\n\nYou can also ask for multiple items by plugging in the combine function.\n\nmy_data[c(3,4), 2]\n\n[1] \"1975\" \"1964\"\n\n\nConsider the command above. Try to formulate it in plain English. What does it tell to do R?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>R Basics</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html",
    "href": "02_data_in_R.html",
    "title": "2  Data in R",
    "section": "",
    "text": "2.1 Class of an object\nR understand the differences between textual and numerical information. We can check the class of an object using the class() function.\n# birth_years contain numerical information\nclass(birth_years)\n\n[1] \"numeric\"\n\n# names_pm contain textual information, which is called character in R\nclass(names_pm)\n\n[1] \"character\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#length-of-an-object",
    "href": "02_data_in_R.html#length-of-an-object",
    "title": "2  Data in R",
    "section": "2.2 Length of an object",
    "text": "2.2 Length of an object\nWe typed names of last eight Prime Ministers and their respective birth years. The number of items in names_pm and birth_years should be both eight. We can see the number of items in a vector by the length() function.\n\n# Number of items in a vector can be seen by length()\n\n# Length of names_pm:\nlength(names_pm)\n\n[1] 8\n\n# Length of birth_years:\nlength(birth_years)\n\n[1] 8",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#is-equal-to-operator",
    "href": "02_data_in_R.html#is-equal-to-operator",
    "title": "2  Data in R",
    "section": "2.3 is equal to operator",
    "text": "2.3 is equal to operator\nYou can ask R whether two things are equal to each other or not. To do so, we are going to use the == operator, which means is equal to.\n\n# is equal to operator: ==\n\n# is the length of names_pm equal to birth_years\nlength(names_pm) == length(birth_years)\n\n[1] TRUE\n\n\nThe number of items in both objects (names_pm and birth_years) is the same because both vectors contain eight pieces of information.\nHow about the class of the objects?\n\n# is the class of names_pm equal to birth_years\nclass(names_pm) == class(birth_years)\n\n[1] FALSE\n\n\nThe class of the objects is not the same because names_pm contains textual information whereas birth_years contains numerical information.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#creating-a-simple-dataset",
    "href": "02_data_in_R.html#creating-a-simple-dataset",
    "title": "2  Data in R",
    "section": "2.4 Creating a simple dataset",
    "text": "2.4 Creating a simple dataset\nLast week we created a simple spreadsheet that looked like the data shown in\nWe can achieve this by doing a column bind which refers to vertically binding two vectors and can be done using the cbind() function.\n\nmy_data &lt;- cbind(names_pm, birth_years)\n\n# let's check the object we created\nmy_data\n\n     names_pm        birth_years\n[1,] \"Keir Starmer\"  \"1962\"     \n[2,] \"Rishi Sunak\"   \"1980\"     \n[3,] \"Liz Truss\"     \"1975\"     \n[4,] \"Boris Johnson\" \"1964\"     \n[5,] \"Theresa May\"   \"1956\"     \n[6,] \"David Cameron\" \"1966\"     \n[7,] \"Gordon Brown\"  \"1951\"     \n[8,] \"Tony Blair\"    \"1953\"     \n\n\nThe first column in my_data is names_pm and the second column is birth_years. We have now two dimensions: columns and rows.\nRecall that to ask R to bring a specific item in a two-dimensional object, such as a spreadsheet, we can use the square-brackets [] notation but we need to specify both dimension.\n\n\n\n\n\n\nRows and Columns in a Spreadsheet\n\n\n\nFirst dimension refers to rows and second dimension refers to columns.\n\n\nLet’s get the third row in second column.\n\n# Third row in second column\nmy_data[3,2]\n\nbirth_years \n     \"1975\" \n\n\nTo sum up, we bound two vectors by column. Each column is a vector. We can call these column vectors. To get the first column, names_pm, we can use the square brackets notation.\n\n# Bring the first column \nmy_data[,1]\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n# Bring the second column\nmy_data[,2]\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n\nTo get a specific column vector, we left the first dimension unspecificed. Recall that the first dimension designates the row, so leaving it unspecified means everything.\nWe could also use column names instead of column numbers.\n\n# Bring the column birth_years\nmy_data[,\"birth_years\"]\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n# Bring the column names_pm\nmy_data[,\"names_pm\"]\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n\nWe can do the same for rows. To get a row vector, use the squared bracket notation.\n\n# Bring the first row\nmy_data[1, ]\n\n      names_pm    birth_years \n\"Keir Starmer\"         \"1962\" \n\n# Bring the third row\nmy_data[3,]\n\n   names_pm birth_years \n\"Liz Truss\"      \"1975\" \n\n# Bring the fourth row\nmy_data[4,]\n\n       names_pm     birth_years \n\"Boris Johnson\"          \"1964\" \n\n\nR will give you an error message if you go out of bounds.\n\n# Bring the third column\nmy_data[,3]\n\nError in my_data[, 3]: subscript out of bounds\n\n# Bring the 10th row\n\n# Bring the second column, ninth row\nmy_data[9,2]\n\nError in my_data[9, 2]: subscript out of bounds",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#data-frame",
    "href": "02_data_in_R.html#data-frame",
    "title": "2  Data in R",
    "section": "2.5 Data frame",
    "text": "2.5 Data frame\nIt is customary to keep a spreadsheet-like looking data (i.e., two-dimensional) as something called a data frame in R. Let’s check the class of my_data.\n\n# Class of my_data\nclass(my_data)\n\n[1] \"matrix\" \"array\" \n\n\nIt looks like the class of my_data is matrix and array. Matrix is a two-dimensional array.\nWe can turn my_data into a data frame.\n\n# Turn my_data into data frame\nmy_data &lt;- as.data.frame(my_data)\n# this just overwrote my_data as a data frame\n\n# Check its class\nclass(my_data)\n\n[1] \"data.frame\"\n\n\nIn this module, we will primarily work with data frames.\nRecall that we can use the $ notation when working with data frames.\n\n# bring names_pm\nmy_data$names_pm\n\n[1] \"Keir Starmer\"  \"Rishi Sunak\"   \"Liz Truss\"     \"Boris Johnson\"\n[5] \"Theresa May\"   \"David Cameron\" \"Gordon Brown\"  \"Tony Blair\"   \n\n# bring birth_years\nmy_data$birth_years\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n# bring the third item in birth_years\nmy_data$birth_years[3]\n\n[1] \"1975\"\n\n\nWe can check the number of columns and the number of rows of our data frame by using ncol() and nrow() functions.\n\n# number of columns\nncol(my_data)\n\n[1] 2\n\n# number of rows\nnrow(my_data)\n\n[1] 8\n\n\nA data frame, such as my_data, has two dimensions: rows and columns. Note that my_data has eight rows and two columns. We can use the dim() function to get the length of each dimension.\n\ndim(my_data)\n\n[1] 8 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#variable-row-observation",
    "href": "02_data_in_R.html#variable-row-observation",
    "title": "2  Data in R",
    "section": "2.6 Variable, Row, Observation",
    "text": "2.6 Variable, Row, Observation\nLet’s check some more terminology that is frequently used in data analysis.\nA column vector typically shows a variable. A row vector typically shows an observation. A particular item, which is a cell in a spreadsheet, is a value. This is visualised in Figure 2.1.\n\n\n\n\n\n\nFigure 2.1: Variables, observations, rows. Well-organised data come in this form.\n\n\n\nWhen data do not come in this format, we will carry out something called data wrangling and reorganize the data so that each column is a variable, each row is an observation and each cell is a value.\nFor simplicity, however, the datasets we are working on already come in this shape.\n\n# A variable: a column (e.g., birth_years)\nmy_data$birth_years\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n# An observation: a row (e.g., second row)\nmy_data[2,]\n\n     names_pm birth_years\n2 Rishi Sunak        1980\n\n# A particular value (e.g., third row of second column)\nmy_data[3,2]\n\n[1] \"1975\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#numerical-value-stored-as-character",
    "href": "02_data_in_R.html#numerical-value-stored-as-character",
    "title": "2  Data in R",
    "section": "2.7 Numerical value stored as character",
    "text": "2.7 Numerical value stored as character\nLet’s say we would like to calculate each person’s current age. We could simply tell R to subtract each birth year from current year (2024).\n\n2024 - my_data$birth_years\n\nError in 2024 - my_data$birth_years: non-numeric argument to binary operator\n\n\nInstead of the calculation, I get an error message: non-numeric argument to…! Let’s see what is going on.\n\n# Check the variable of interest\nmy_data$birth_years\n\n[1] \"1962\" \"1980\" \"1975\" \"1964\" \"1956\" \"1966\" \"1951\" \"1953\"\n\n\nbirth_years is a vector of numbers but if you look closely, you will see that each number is shown within a pair of quotation mark. This is because R is keeping each number as text at the moment. Let’s look at the class of the object.\n\n# Class of birth_years\nclass(my_data$birth_years)\n\n[1] \"character\"\n\n\nCharacter means text. We are going to use as.numeric() function to tell R that information stored in birth_years is numerical, not text.\n\n# Convert the variable to numerical\nas.numeric(my_data$birth_years)\n\n[1] 1962 1980 1975 1964 1956 1966 1951 1953\n\n\nNow quotation marks disappeared. Beware: I have not overwritten the variable yet. It is just displayed on my screen for one time only. I need to overwrite the existing version to make it a permanent change.\n\nmy_data$birth_years &lt;- as.numeric(my_data$birth_years)\n\nThis command tells R to:\n\ngo and get the variable birth_years inside the data frame my_data\nconvert it numeric\ntake the numerical output and assign it over the variable birth_years in the data frame my_data\n\nNow birth_years should be numerical.\n\nclass(my_data$birth_years)\n\n[1] \"numeric\"\n\n\nNow, we can create the current age variable.\n\n# Calculate the current age\n2023 - my_data$birth_years\n\n[1] 61 43 48 59 67 57 72 70\n\n# It is working. Let's assign this output to a new variable\nmy_data$age_current &lt;- 2023 - my_data$birth_years\n\n# Check my_data\nmy_data\n\n       names_pm birth_years age_current\n1  Keir Starmer        1962          61\n2   Rishi Sunak        1980          43\n3     Liz Truss        1975          48\n4 Boris Johnson        1964          59\n5   Theresa May        1956          67\n6 David Cameron        1966          57\n7  Gordon Brown        1951          72\n8    Tony Blair        1953          70",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#categorical-data",
    "href": "02_data_in_R.html#categorical-data",
    "title": "2  Data in R",
    "section": "2.8 Categorical data",
    "text": "2.8 Categorical data\nI would like to add a variable showing the party of each Prime Minister. I can create a vector and add this as a column in my_data.\nFor example, Rishi Sunak is from Conservative Party, Liz Truss is also Conservative. Kier Starmer, Gordon Brown and Tony Blair are Labour.\nWe need a vector where the first item is Labour, followed by five Conservative, and two Labour at the end.\nWe could write it one by one in order. It would be long and cumbersome, but it would do the job.\n\nparties_long_version &lt;-  c(\"Labour\", # first item Labour\n                                \"Conservative\", # followed by five Conservative (1)\n                                \"Conservative\", # (2)\n                                \"Conservative\", # (3)\n                                \"Conservative\", # (4)\n                                \"Conservative\", # (5)\n                                \"Labour\", # This corresponds to Gordon Brown\n                                \"Labour\" # Finally, Tony Blair\n                                )\n\nLet’s check the object we created.\n\nparties_long_version\n\n[1] \"Labour\"       \"Conservative\" \"Conservative\" \"Conservative\" \"Conservative\"\n[6] \"Conservative\" \"Labour\"       \"Labour\"      \n\n\nThis looks good. I could assign it into a new column in my_data. But we are learning, so let’s try another and faster-to-write way.\nInstead of repeating Conservative five times, I could use the repeat function: rep().\n\n# rep() repeats an input n times\nrep(\"Conservative\", 5)\n\n[1] \"Conservative\" \"Conservative\" \"Conservative\" \"Conservative\" \"Conservative\"\n\n\nUsing this approach, I can build the vector again. I need one “Labour”, five “Conservative”, and two “Labour”, in this order. I need to combine them using (c).\n\nparties_short_version &lt;-  c(\"Labour\",\n                            rep(\"Conservative\", 5),\n                            rep(\"Labour\", 2))\n\nLet’s check the object we created\n\nparties_short_version\n\n[1] \"Labour\"       \"Conservative\" \"Conservative\" \"Conservative\" \"Conservative\"\n[6] \"Conservative\" \"Labour\"       \"Labour\"      \n\n\nBoth versions should be the same, meaning parties_long_version should be equal to parties_short_version.\n\nparties_long_version == parties_short_version \n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nNow, let’s put it into my data frame.\n\nmy_data$party &lt;- parties_short_version\n\nLet’s check the data frame.\n\nmy_data\n\n       names_pm birth_years age_current        party\n1  Keir Starmer        1962          61       Labour\n2   Rishi Sunak        1980          43 Conservative\n3     Liz Truss        1975          48 Conservative\n4 Boris Johnson        1964          59 Conservative\n5   Theresa May        1956          67 Conservative\n6 David Cameron        1966          57 Conservative\n7  Gordon Brown        1951          72       Labour\n8    Tony Blair        1953          70       Labour",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#counting-frequencies-using-table",
    "href": "02_data_in_R.html#counting-frequencies-using-table",
    "title": "2  Data in R",
    "section": "2.9 Counting frequencies using table ()",
    "text": "2.9 Counting frequencies using table ()\nLet’s see how many individuals from each party is in my data frame. You could count it one by one in a small data set such as this one. I can see that there are 5 Conservatives and three Labour, but imagine that it was a large dataset where counting manually was not an option.\nWe can use table() function to achieve this.\n\ntable(my_data$party)\n\n\nConservative       Labour \n           5            3",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#saving-data",
    "href": "02_data_in_R.html#saving-data",
    "title": "2  Data in R",
    "section": "2.10 Saving data",
    "text": "2.10 Saving data\nIn the final step, we will learn how to save a data frame such as my_data for future use. We have a few options:\n\nWrite my_data into a spreadsheet-like file.\nSave the whole R environment with all the objects inside.\n\nWe will cover option #1 here.\nYou have probably used Microsoft Excel (or Google Sheets) to work on spreadsheets before. There are different spreadsheet file types (such as Excels .xlsx), but the most common and compatible one is .csv, which stands for comma separated values. This is basically plain text that any computer and most electronic devices can open.\n\nwrite.csv(x = my_data, file = \"my_first_file.csv\", row.names = F)\n\nThis should create a file somewhere in your computer, more precisely, in your working directory. Let’s see where it has saved the file by looking at the working directory.\n\ngetwd()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "02_data_in_R.html#working-directory",
    "href": "02_data_in_R.html#working-directory",
    "title": "2  Data in R",
    "section": "2.11 Working directory",
    "text": "2.11 Working directory\ngetwd() means get working directory. Working directory is your default file path. This is where R looks for files and saves any output.\nWorking directory can be different in each computer. R Studio has nice tools for navigation.\nYou can directly go to your working directory through Files tab (usually in right bottom corner) and using the More drop-down menu. Under there there are a few options:\n\nSet as working directory: sets your working directory as the current directory shown in Files\nGo to working directory: takes you to current working directory\n\nIf you click on go to working directory, you should see my_first_file.csv here.\nIt is a good idea to create a new directory (folder for Windows) for this module. Your folder names should be simple and easy to write. For example: research_methods is a good name.\nTry not to use space in file names. Underscore or dash are better alternatives. Also, I encourage always using lowercase for file names, which also goes for object names in R.\nYou can use your operating system to create this directory. You could also use R Studio’s Files tab. Put this folder somewhere easy to access.\nLet’s save your R script. You can use drop-down menu: File &gt;&gt; Save OR simply by using the keyboard shortcut .\nGive an intuitive name to your script. For example, learn_02.R is a good name.\nIt is generally good idea to keep your data in a sub-directory named data. Create such a directory and move my_first_file.csv there.\nNext week, we will continue with a simple dataset, which will be available on Blackboard.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data in R</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html",
    "href": "03_data_analysis.html",
    "title": "3  Data Basics",
    "section": "",
    "text": "3.1 Load a .csv file\nFor practice purposes, let’s load the simple dataframe we created last week. We will use read.csv() function. Recall that the file is under the folder data.\nLet’s download the dataset names World in 2010. This dataset is available on Blackboard. It can also be downloaded from here. A codebook is also available on Blackboard. Make sure to take a look at it\nLet’s be tidy and move this file into the data folder under our module folder. You can navigate in your module directory within R Studio. For example, the files tab in Figure 3.2 show the folder data.\nYou can actually list files in your computer with R commands too!\n# list files under working directory\nlist.files()\n# list files under data folder \nlist.files(\"data/\")\nNow we are ready to load world_in_2010.csv into R. We should try to understand the structure of the dataset. Let’s have a good sense of this dataset.\n# load data:\ndf &lt;- read.csv(\"data/world_in_2010.csv\")\n\n# have a look using View(df)\n# View(df)\n\n# type of the object\nclass(df)\n\n[1] \"data.frame\"\n\n#&gt; [1] \"data.frame\"\n\n# how many variables?\nncol(df)\n\n[1] 42\n\n#&gt; [1] 42\n\n# how many rows?\nnrow(df)\n\n[1] 166\n\n#&gt; [1] 166\n\n# names of the variables\nnames(df)\n\n [1] \"COWcode\"                             \"Country_Code\"                       \n [3] \"Country_Name\"                        \"WB_Region\"                          \n [5] \"WB_IncomeGroup\"                      \"Population_total\"                   \n [7] \"Urban_pop\"                           \"GDP_pc_PPP\"                         \n [9] \"Infant_Mortality_Rate\"               \"Life_exp_female\"                    \n[11] \"Life_exp_male\"                       \"HIV\"                                \n[13] \"Literacy_rate_female\"                \"Literacy_rate_all\"                  \n[15] \"Current_acc_bal_USD\"                 \"Current_acc_bal_perc_of_GDP\"        \n[17] \"ODA_USD\"                             \"ODA_perc_of_GNI\"                    \n[19] \"Natural_resources_rents_perc_of_GDP\" \"FDI_net_inflows_perc_of_GDP\"        \n[21] \"Net_migration_2008_2012\"             \"GINI_index_WB_estimate\"             \n[23] \"Inc_share_by_highest_10per\"          \"Unemployment_rate\"                  \n[25] \"Surface_area_sq_km\"                  \"v2x_polyarchy\"                      \n[27] \"democracy\"                           \"v2x_libdem\"                         \n[29] \"v2x_egaldem\"                         \"Geographical_Region\"                \n[31] \"UN_vote_PctAgreeUS\"                  \"UN_vote_PctAgreeRUSSIA\"             \n[33] \"UN_vote_PctAgreeBrazil\"              \"UN_vote_PctAgreeChina\"              \n[35] \"UN_vote_PctAgreeIndia\"               \"UN_vote_PctAgreeIsrael\"             \n[37] \"milex\"                               \"milper\"                             \n[39] \"cinc\"                                \"CivilConflict\"                      \n[41] \"Corruptions_Perspectives_Index\"      \"Turnout\"\nEach row represents a state in the international system. So we can say that the unit of observation is the state. Variables show several attributes of each state (note that I will use state and country interchangeable).\nWe call this a cross-sectional data because we have units but no time dimension. The whole dataset is for the year 2010. Usually, such country-level data would also contain multiple years so that comparison can be over-time, but to keep things simple for now, we are only working with a single year. When we add the time dimension to cross-sectional data, we will call it time-series cross-sectional.\nHow many countries do we have in this dataset? We already know the answer because we checked the number of rows. Recall that each row represents a state in the international system so that the total number of rows will give me the number of countries in the data.\n# how many rows?\nnrow(df)\n\n[1] 166",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#load-a-.csv-file",
    "href": "03_data_analysis.html#load-a-.csv-file",
    "title": "3  Data Basics",
    "section": "",
    "text": "Figure 3.2: R Studio screen",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#summary-of-a-categorical-variable",
    "href": "03_data_analysis.html#summary-of-a-categorical-variable",
    "title": "3  Data Basics",
    "section": "3.2 Summary of a categorical variable",
    "text": "3.2 Summary of a categorical variable\nLet’s describe a categorical variable. We can work with Geographical_Region, which records where a country is located geographically. To see how many countries are in each region, we can use table() and provide a descriptive summary of this variable.\n\ntable(df$Geographical_Region)\n\n\n                  Africa                 Americas                     Asia \n                      47                       27                       30 \n                  Europe Mid. East & North Africa                   Ocenia \n                      39                       17                        6 \n\n\nNow, we have a frequency table of Geographical_Region. I can see that 47 countriese are in Africa, 27 in Americas and so on. For categorical variables, a frequency table is appropriate for a descriptive summary.\nWe can also create a bar plot and visually summarize the data. Each bar will represent the number of countries in each geographical region. In short, we will visually display the information in table(df$Geographical_Region) using a barplot.\n\nbarplot(height = table(df$Geographical_Region))\n\n\n\n\n\n\n\n\nThis is a good start, but we will do better. Before going any further, let’s unpack the code. The function barplot() takes a vector of numbers, which it uses to display heights. For instance, if we want to display four bars with heights 5, 10, 12 and 7, we can plug such a vector into barplot().\n\nbarplot(height = c(5,10,12,7))\n\n\n\n\n\n\n\n\nNote that bars above don’t have labels at the moment, because we did not provide any information. For Geographical_Region, however, table() creates a named vector.\n\n# df$Geographical_Region is a named vector:\ntable(df$Geographical_Region)\n\n\n                  Africa                 Americas                     Asia \n                      47                       27                       30 \n                  Europe Mid. East & North Africa                   Ocenia \n                      39                       17                        6 \n\n# Names:\nnames(table(df$Geographical_Region))\n\n[1] \"Africa\"                   \"Americas\"                \n[3] \"Asia\"                     \"Europe\"                  \n[5] \"Mid. East & North Africa\" \"Ocenia\"                  \n\n\nGoing back to our hypothetical barplot, we can provide it a names argument. Let’s say the numbers 5, 10, 12 and 7 correspond to Orange, Banana, Apple, Pear.\n\nbarplot(height = c(5,10,12,7), names.arg = c(\"Orange\", \"Banana\", \"Apple\", \"Pear\"))\n\n\n\n\n\n\n\n\nFor easier read and navigation, it is a good idea to use a systematic way to write the code. You can press  after the end of the first argument.\n\n# easier to read code\nbarplot(height = c(5,10,12,7), \n        names.arg = c(\"Orange\", \"Banana\", \"Apple\", \"Pear\")\n        )\n\n\n\n\n\n\n\n\nNow we have a very good idea how barplot() works.\nGoing back to the bar plot for Geographical_Region, the category “Mid. East & North Africa” is too long. I want to change it with just “MENA”.\nYou can take assign table input to an object and work on it.\n\n# Get the table() into an object\ngeoreg_tab &lt;- table(df$Geographical_Region)\n\n# Check the object\ngeoreg_tab \n\n\n                  Africa                 Americas                     Asia \n                      47                       27                       30 \n                  Europe Mid. East & North Africa                   Ocenia \n                      39                       17                        6 \n\n# check the names of each value\nnames(georeg_tab)\n\n[1] \"Africa\"                   \"Americas\"                \n[3] \"Asia\"                     \"Europe\"                  \n[5] \"Mid. East & North Africa\" \"Ocenia\"                  \n\n\nFifth item in names(georeg_tab) is “Mid. East & North Africa”. I could go and change it by using the squared bracket [] notation.\n\n# Fifth Item in Barplot\nnames(georeg_tab)[5]\n\n[1] \"Mid. East & North Africa\"\n\n# To change it you can assign a new name\nnames(georeg_tab)[5] &lt;- \"MENA\"\n\n# Check the object again\ngeoreg_tab\n\n  Africa Americas     Asia   Europe     MENA   Ocenia \n      47       27       30       39       17        6 \n\n\nNow we are ready to do the barplot.\n\nbarplot(georeg_tab)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#understanding-functions",
    "href": "03_data_analysis.html#understanding-functions",
    "title": "3  Data Basics",
    "section": "3.3 Understanding functions",
    "text": "3.3 Understanding functions\nWe used the command barplot() to create a bar graph. barplot() is called a function. A function usually gets an object (or objects), and returns something, such as a graph.\nYou can use the help() function to read more about a specific function. For example, let’s read the help file for barplot().\n\nhelp(barplot)\n\n\n\n\n\n\n\nFigure 3.3: Help tab explaining barplot()\n\n\n\nAn input of a function is called an argument. We don’t need to specify all arguments, but some of them are always required. For example, for barplot(), we need to define the height.\n\nbarplot(height = c(5, 10, 15))\n\n\n\n\n\n\n\n\nWe can add names by adding names.arg.\n\nbarplot(height = c(5, 10, 15),\n        names.arg = c(\"Banana\", \"Apple\", \"Pear\"))\n\n\n\n\n\n\n\n\nYou can change the width of each bar.\n\nbarplot(height = c(5, 10, 15),\n        names.arg = c(\"Banana\", \"Apple\", \"Pear\"),\n        width = c(1,2,3))\n\n\n\n\n\n\n\n\nIt is also possible to change colors (for all three, or one by one).\n\n# yellow, red and green colors\nbarplot(height = c(5, 10, 15),\n        names.arg = c(\"Banana\", \"Apple\", \"Pear\"),\n        col = c(\"yellow\", \"red\", \"green\"))\n\n\n\n\n\n\n\n\n\n# all blue\nbarplot(height = c(5, 10, 15),\n        names.arg = c(\"Banana\", \"Apple\", \"Pear\"),\n        col = \"blue\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#summary-of-a-numerical-variable",
    "href": "03_data_analysis.html#summary-of-a-numerical-variable",
    "title": "3  Data Basics",
    "section": "3.4 Summary of a numerical variable",
    "text": "3.4 Summary of a numerical variable\nNext, let’s work with a numerical variable. Life_exp_female is a measurement of female life expectancy across the world in 2010. The function summary() will give the min, max, mean, median, and first and third quartiles.\n\n# Summary of the variable\nsummary(df$Life_exp_female)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  48.88   64.41   74.91   71.94   78.73   86.30 \n\n\nWe can also check such statistics one by one.\n\n# mean of a variable\nmean(df$Life_exp_female)\n\n[1] 71.94012\n\n# minimum\nmin(df$Life_exp_female)\n\n[1] 48.88\n\n# maximum\nmax(df$Life_exp_female)\n\n[1] 86.3\n\n\nRecall that quartiles divides the data into four parts. Median is the mid-point, which is also called the second quartile.\n\nFive point data summary\n\n\n\n\n\n\n\n\nSymbol\nName(s)\nDefinition\nUse\n\n\n\n\nmin\nMinimum\nMinimum of data\nThe lowest data point\n\n\nQ1\n25th Percentile\nFirst Quartile\nSplits off the lowest 25% of data from the highest 75%\nA common low value\n\n\nQ2\n50th Percentile\nSecond Quartile\nMedian\nMiddle of data\nA common value\n\n\nQ3\n75th Percentile\nThird Quartile\nsplits off the highest 75% of data from the lowest quarter\nA common high value\n\n\nmax\nMaximum\nMaximum of data\nThe highest data point\n\n\n\nBesides summary() we can also use median() and quantile() functions to get the quartiles.\n\n# median:\nmedian(df$Life_exp_female)\n\n[1] 74.905\n\n# also median:\nquantile(df$Life_exp_female, probs = 0.50) # 0.50 indicates half-way (50%)\n\n   50% \n74.905 \n\n# first quartile\nquantile(df$Life_exp_female, probs = 0.25) # 0.25 indicates 25%\n\n  25% \n64.41 \n\n# third quartile\nquantile(df$Life_exp_female, probs = 0.75) # 0.75 indicates 75%\n\n  75% \n78.73 \n\n# five point summary\nquantile(df$Life_exp_female, probs = c(0, 0.25, 0.50, 0.75, 1))\n\n    0%    25%    50%    75%   100% \n48.880 64.410 74.905 78.730 86.300 \n\n\nUsing summary() to get a five-point numerical summary is perfectly fine. We briefly visited quantile() for demonstration purposes. There are many ways to achieve the same thing in R.\nFor dispersion, variance and standard deviation can be calculated.\n\n# standard deviation\nsd(df$Life_exp_female)\n\n[1] 9.353926\n\n# variance\nvar(df$Life_exp_female)\n\n[1] 87.49594\n\n# recall that square root of variance is standard deviation\nsqrt( var(df$Life_exp_female) )\n\n[1] 9.353926\n\n# ask R if you don't believe me\nsd(df$Life_exp_female) == sqrt( var(df$Life_exp_female) )\n\n[1] TRUE\n\n\nNote that == here means is equal to?\n\n# 2 plus 2 is 4, or is it?\n2 + 2 == 4\n\n[1] TRUE\n\n# is 10 / 2 equal to 4?\n10 / 2 == 4 \n\n[1] FALSE\n\n# no",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#visual-summary",
    "href": "03_data_analysis.html#visual-summary",
    "title": "3  Data Basics",
    "section": "3.5 Visual summary",
    "text": "3.5 Visual summary\nWhen working on a new dataset, it is always a good idea to start by visually summarizing your variables of interest one-by-one. This will help you to get a better understanding of the data.\nFor a numerical variable, two types of graphs are appropriate for a visual summary:\n- Histogram\n- Box plot\n\n3.5.1 Histogram\n\n# histogram:\nhist(df$Life_exp_female)\n\n\n\n\n\n\n\n\nYou can tell R how many bins you would like to have in your histogram by using the breaks argument, but as the R help file clarifies, this number is a suggestion only, and R can use a different (but similar value) to draw a pretty histogram.\n\n# histogram with fewer bins\nhist(df$Life_exp_female, breaks = 5)\n\n\n\n\n\n\n\n\n\n# histogram with higher number of bins\nhist(df$Life_exp_female, breaks = 30)\n\n\n\n\n\n\n\n\nYou might want to specify cut points. For example, you might want to have a sequence from 40 to 100 with increment of 5.\n\n# a sequence from 40 to 100 by 5\nseq(from = 40, to = 100, by = 5)\n\n [1]  40  45  50  55  60  65  70  75  80  85  90  95 100\n\n# put it into an object\nmy_breaks &lt;- seq(from = 40, to = 100, by = 5)\n# this will be our break points\n\n#tell R to do the histogram using these points\nhist(df$Life_exp_female, breaks = my_breaks)\n\n\n\n\n\n\n\n\nThis looks quite nice and intuitive. However, titles are not defined. I don’t want to see df$Life_exp_female as an axis or main graph title.\n\n#Histogram with titles and breaks\nhist(df$Life_exp_female, \n     breaks = my_breaks, \n     main = \"Female Life Expectancy in 2010\",\n     ylab = \"Number of Countries\",\n     xlab = \"Age\"\n     )\n\n\n\n\n\n\n\n\n\n\n3.5.2 Box plots\nA box plot is another option for graphically summarising a numerical variable. Box graphs are really nice to get a sense of the data, understand the distribution, and quickly see if there are any outliers. They are also very good at creating visual comparisons across groups, something which we will cover in the upcoming weeks.\n\n# Boxplot\nboxplot(df$Life_exp_female,\n        main = \"Female Life Expectancy in 2010\",\n        ylab = \"Age\")\n\n\n\n\n\n\n\n\nThis box plot helps us to visualize the five-point summary. You can see minimum, max, median and first and third quartiles.\nIf you don’t believe me, we can plot these over the boxplot.\n\n# box plot:\nboxplot(df$Life_exp_female,\n        main = \"Female Life Expectancy in 2010\",\n        ylab = \"Age\")\n\n# you can add a line to a plot by abline\n# h stands for horizontal\n# tell R to draw a horizontal line at the median of Life_exp_female\nabline(h = median(df$Life_exp_female))\n\n\n\n\n\n\n\n\nYou can draw lines for each statistic.\n\n# box plot:\nboxplot(df$Life_exp_female,\n        main = \"Female Life Expectancy in 2010\",\n        ylab = \"Age\")\n\n# median\nabline(h = median(df$Life_exp_female))\n\n# min\nabline(h = min(df$Life_exp_female))\n\n# max\nabline(h = max(df$Life_exp_female))\n\n# first quartile\nabline(h = quantile(df$Life_exp_female, probs = 0.25))\n\n# third quartile\nabline(h = quantile(df$Life_exp_female, probs = 0.75))\n\n\n\n\n\n\n\n\nWe drew these lines for demonstration purposes only. It is to learn that a box plot visualizes the information in a five-point summary. Normally, we wouldn’t show them this way because we can assume the reader knows how to read a box plot.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#dealing-with-missingness",
    "href": "03_data_analysis.html#dealing-with-missingness",
    "title": "3  Data Basics",
    "section": "3.6 Dealing with missingness",
    "text": "3.6 Dealing with missingness\nSometimes our variable of interest may have missing values inside. For example, GDP per capita PPP (PPP stands for purchasing power parity) variable is GDP_pc_PPP. GDP data for some countries are missing.\n\nsummary(df$GDP_pc_PPP)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n   646.9   3127.0   9627.9  15494.4  20497.9 122609.4        5 \n\n\nNA’s here means missing. GDP data for five countries are not available. Let’s find these five countries. The function is.na() tells whether something is missing or not.\n\n# is there any missing values in GDP_pc_PPP?\nis.na(df$GDP_pc_PPP)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE\n [37] FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nWhat are the names of the countries with missing GDP_pc_PPP? We want to tell R the following sentence: bring ‘Country Name’ such that ‘GDP_pc_PPP’ is missing. We will use square brackets notation, [], to create this sentence.\n\n# Bring the values of df$Country_Name for those where df$GDP_pc_PPP is not available \ndf$Country_Name[is.na(df$GDP_pc_PPP) == TRUE]\n\n[1] \"Cuba\"       \"Djibouti\"   \"Korea, Dem\" \"Somalia\"    \"Syrian Ara\"\n\n\nDealing with missingness appropriately is important because it might create problems. For example, regular functions mean() and max() will get confused if we do not deal with missingness explicitly.\n\n# mean of GDP_pc_PPP\nmean(df$GDP_pc_PPP)\n\n[1] NA\n\n\nR tells me it is missing because you cannot make a calculation with a missing value. We need to tell R to remove the missing value\n\n# mean of GDP_pc_PPP (remove missing values)\nmean(df$GDP_pc_PPP, na.rm = TRUE)\n\n[1] 15494.4\n\n\nThis is also the case for other functions such as min() and max().\n\n# max without na.rm\nmax(df$GDP_pc_PPP)\n\n[1] NA\n\n\n\n# max with na.rm\nmax(df$GDP_pc_PPP, na.rm = T)\n\n[1] 122609.4\n\n\nInstead of TRUE or FALSE you ca write T and F as a shortcut. These are reserved characters in R, standing for TRUE and FALSE, respectively.\n\nmin(df$GDP_pc_PPP, na.rm = T)\n\n[1] 646.86\n\n\n\nmax(df$GDP_pc_PPP, na.rm = T)\n\n[1] 122609.4\n\n\nGraphical functions will handle missingness automatically.\n\n# histogram\nhist(df$GDP_pc_PPP)\n\n\n\n\n\n\n\n\n\n# boxplot\nboxplot(df$GDP_pc_PPP)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "03_data_analysis.html#using-square-brackets-for-further-investigation",
    "href": "03_data_analysis.html#using-square-brackets-for-further-investigation",
    "title": "3  Data Basics",
    "section": "3.7 Using square brackets for further investigation",
    "text": "3.7 Using square brackets for further investigation\nThis final box plot above (for GDP per capita) shows six points above the vertical line denoting the maximum. What are those?\nThey are called outliers. These countries have a very high GDP per capita. These values are quite different than the rest, as they are much higher. Such values are called outliers.\nLet’s find which countries have these extremely high values. Recall that we can use square brackets notation, [], to tell R which values to bring. The pseudocode is the following:\ndf$Country_Name[ 'ROW NUMBER OF THE HIGHEST GDP PER CAPITA']\nTo write this, I need the know the row number of the observation with the highest GDP per capita.\nI am going to sort my data frame by using order(), which will give me the row number based on ordering of a variable.\n\n# row numbers by highest GDP_pc_PPP\norder(df$GDP_pc_PPP, decreasing = T)\n\n  [1] 124  82 131 111  24 158 110 128 114  68  39   7 141  10  23   6  37  48\n [19]  71  50  52  73  76  35  58  45 113 151  81  86  72  59 140  36 122 139\n [37]  46  65 120 105 126  89 143  77  63   4  25  69  90 153 125 157 160  84\n [55]  19  16   8 103 116  51  14  95  18 138  99 147  21  33  41  70 136  94\n [73] 164  96  40  31 152 117 149  43   3 123  26  15  75  42 150  87 106  66\n [91]  74 162 156 100  49 142  53  60   5  91  20   2 134  61 118  17  30 108\n[109] 163  67 115 159  83 109  62  92  98 129 102 165 119  54 161  28  79 130\n[127]  27 137  80  78  13  32  88 148 154 112 145  97 132  11 166   1  56  55\n[145] 155  64  12  44  57  93 127 146 133  85  47 104 101  22 107   9  29  34\n[163]  38 121 135 144\n\n\nLet’s store these row numbers in an object. This way we don’t need to write this long statement again and again.\n\n# row numbers by highest GDP_pc_PPP stored in row_nos\nrow_nos &lt;- order(df$GDP_pc_PPP, decreasing = T)\n\nrow_nos are the row numbers of when the data is ordered from the highest GDP per capita to the lowest.\nFirst item in row_nos is the row number of the observation with the highest GDP_pc_PPP.\n\n# row no of the observation with the highest GDP per capita\nrow_nos[1]\n\n[1] 124\n\n\nLet’s get the name of the country! Recall that I can use the square brackets notation.\n\n# name of the country with the highest GDP per capita\ndf$Country_Name[ row_nos[1] ]\n\n[1] \"Qatar\"\n\n\nQatar has the highest GDP per capita in 2010. But I am interested in the highest 6 countries.\n\n# recall that to get the first six rows, you could write this:\nrow_nos[c(1,2,3,4,5,6)]\n\n[1] 124  82 131 111  24 158\n\n\nBut writing c(1,2,3,4,5,6) is cumbersome. A better approach is using seq() function.\n\n# sequence from 1 to 6\nseq(1, 6)\n\n[1] 1 2 3 4 5 6\n\n\nR has a shortcut for creating a sequence.\n\n# sequence from 1 to 6: shortest version\n1:6\n\n[1] 1 2 3 4 5 6\n\n\nSo, seq(1,6) and 1:6 means exactly the same thing in R.\n\n# are the items in 1:6 equal to the items in seq(1,6)\n1:6 == seq(1,6)\n\n[1] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nNow, we can write the phrase for getting the row numbers simpler. We will also need to plug it into df$Country_Name using square brackets.\n\n# Go to df$Country_Name and bring me these observations\n# 'these' refers to row_nos[1:6]\ndf$Country_Name[ row_nos[1:6] ]\n\n[1] \"Qatar\"      \"Kuwait\"     \"Singapore\"  \"Norway\"     \"Switzerlan\"\n[6] \"United Sta\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Basic Data Analysis</span>"
    ]
  },
  {
    "objectID": "04_pipe.html",
    "href": "04_pipe.html",
    "title": "4  Functions, Libraries and Operators",
    "section": "",
    "text": "4.1 Pipe operator\nWe often put one function inside another function. For example, you are trying to find the mean of your exam scores.\n# Your exam scores:\nc(65,68,71,74, 53, 58)\n\n[1] 65 68 71 74 53 58\n\n# Mean of the exam score:\nmean(c(65,68,71,74, 53, 58))\n\n[1] 64.83333\nIn the example above, you combine your exam scores using c() and then put this into mean(). In other words, the output of c() becomes the input of mean().\nSometimes we need to repeatedly put functions inside of other functions to create long chains. In such cases, reading and writing code might become cumbersome.\nFor example, let’s say that only the highest five exam scores are going to be considered.\n# Your exam scores:\nc(65,68,71,74, 53, 58)\n\n[1] 65 68 71 74 53 58\n\n# Your exam scores sorted: \nsort(c(65,68,71,74, 53, 58), decreasing = T)\n\n[1] 74 71 68 65 58 53\n\n# Your exam scores sorted and highest five selected\nhead(sort(c(65,68,71,74, 53, 58), decreasing = T), n = 5)\n\n[1] 74 71 68 65 58\n\n# the mean of highest five\nmean(head(sort(c(65,68,71,74, 53, 58), decreasing = T), n = 5))\n\n[1] 67.2\nThere is a four step chain, which is visualised below.\nflowchart LR\n  A(combine) --&gt; B(sort highest to lowest) --&gt; C(take the highest five exam scores) --&gt; D(take the mean)\nWe can use the operator |&gt; to avoid writing one function inside another. |&gt; is called the pipe operator.\nc(65,68,71,74, 53, 58) |&gt; sort(decreasing = T) |&gt; head(n = 5) |&gt; mean()\n\n[1] 67.2\nThis makes writing and reading the code easier.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Libraries and Operators</span>"
    ]
  },
  {
    "objectID": "04_pipe.html#creating-functions",
    "href": "04_pipe.html#creating-functions",
    "title": "4  Functions, Libraries and Operators",
    "section": "4.2 Creating functions",
    "text": "4.2 Creating functions\nR comes with many functions, but you can write your own functions as well.\nFor example, mean() calculates the mean of a numerical vector. You could also calculate it by taking the sum of numbers and dividing it by n.\n\n# some temperature readings of a room\ntemps &lt;- c(19, 21, 17, 24, 15)\n\n# mean of temperatures using the mean function\nmean(temps)\n\n[1] 19.2\n\n# calculating the mean by 'hand'\nsum(temps)/length(temps)\n\n[1] 19.2\n\n\nWe can create a function that takes a vector of numbers, sums them up and divides it by the length of the vector.\n\nfunction(x) {\n  sum(x) /  length(x)\n  }\n\nHere x is a placeholder for the input. The function has one input, x, which is summed up and divided by its length.\n\n# keep this function as my_function \nmy_function &lt;- function(x) {\n  sum(x) /  length(x)\n  }\n\nLet’s use our tailor-made function.\n\nmy_function(temps)\n\n[1] 19.2\n\n\nAs you can see, it is giving the exactly same result.\nFunctions can have more than one input.\n\n# another function called my_other_function\nmy_other_function &lt;- function(input_one, input_two) {\n  input_one * input_two / (input_one + input_two)\n}\n\n# \nmy_other_function(10, 40)\n\n[1] 8\n\n# my_other_function(10, 40) is equivalent of:\n\n(10 * 40) / (10 + 40)\n\n[1] 8\n\n#&gt; [1] 8\n\nmy_other_function gets two inputs, input_one and input_two, their multiplication is divided by their sum. Mathematically, my other function is equivalent to the function below:\n\\[\nf(x,y) = \\frac{x * y}{x + y}\n\\]\nFor example, let’s put 10 and 40 for respective inputs.\n\n# calculating by typing\n(10 * 40) / (10 + 40)\n\n[1] 8\n\n# calculating by using my_other_function\nmy_other_function(10, 40)\n\n[1] 8\n\n\n\n4.2.1 Challenge: finding the mode\nRecall that the mode is the most frequent observation. R does not have an in-built function to display the mode. Instead we can create our own function.\nLet’s create pet data and store the fruits I ate last week\n\n# fruits I ate last week:\nfruits &lt;- c(\"banana\", \"apple\", \"banana\", \"mango\", \"banana\", \"watermelon\", \"apple\")\n\nLet’s see the frequencies.\n\n# frequencies of fruits\ntable(fruits)\n\nfruits\n     apple     banana      mango watermelon \n         2          3          1          1 \n\n\nIt is east to see that banana is the mode because it has the highest value.\nBut it is not enough because I want to generalize this. To ask R to bring me the mode, I need to construct the following command:\n\ntable(fruits)[ table(fruits) == max(table(fruits)) ] \n\nbanana \n     3 \n\n\nThis is hard to process. Let’s unpack it, starting from the inside.\n\n# maximum of the table\nmax(table(fruits))\n\n[1] 3\n\n\nNow I know that the frequency of the most common value. Next, let’s ask R if a category has the highest frequency.\n\ntable(fruits) == max(table(fruits))\n\nfruits\n     apple     banana      mango watermelon \n     FALSE       TRUE      FALSE      FALSE \n\n\nFor each category, R checks if the category is the most common. Finally, I will tell R to bring only the most common category.\n\ntable(fruits)[ table(fruits) == max(table(fruits)) ] \n\nbanana \n     3 \n\n\nIf you write this in plain English, we are telling R to make a frequency table of fruits, and from that table bring only the category that is most common. You can generalize this.\n\n# home-made function for finding the mode\nmode_function &lt;- function(x){\n    table(x)[ table(x) == max(table(x)) ] \n    }\n\nLet’s use it.\n\nmode_function(fruits)\n\nbanana \n     3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Libraries and Operators</span>"
    ]
  },
  {
    "objectID": "04_pipe.html#libraries",
    "href": "04_pipe.html#libraries",
    "title": "4  Functions, Libraries and Operators",
    "section": "4.3 Libraries",
    "text": "4.3 Libraries\nR community is large and active. Many clever people are creating their own additions to R, just like we created the mode_function(). For example, there is an addition to R called DescTools which comes with a function that calculates the mode of the data.\nThese additions are called libraries. You can think them as mini-applications within R.\nLet’s install and use DescTools.\n\ninstall.packages(\"DescTools\")\n\nYou need to install a package only once, but you need to recall it in each session.\n\nlibrary(DescTools)\n\n\n# Mode of fruits (using DescTools Mode() function)\nMode(fruits)\n\n[1] \"banana\"\nattr(,\"freq\")\n[1] 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Libraries and Operators</span>"
    ]
  },
  {
    "objectID": "04_pipe.html#us-presidents-approval-data",
    "href": "04_pipe.html#us-presidents-approval-data",
    "title": "4  Functions, Libraries and Operators",
    "section": "4.4 US Presidents Approval Data",
    "text": "4.4 US Presidents Approval Data\nRecall that we discussed the approval rating of last US Presidents. Let’s use this data. It is available here and also on Blackboard.\nLet’s remove everything on your environment and import the data.\n\n# remove everything from the environment\nrm(list = ls())\n\n# load the data\ndf &lt;- read.csv(\"data/approval_data.csv\")\n\nVisually see the data on your screen:\n\n# View(df)\nView(df)\n\nThis is a time-series cross-sectional dataset, meaning that there are multiple cases measured over a time period. It is cross-sectional because there are multiple US Presidents. It is time-series because there are measurements over time. Unit of observation is President-Time, making it a time-series dataset.\nLet’s continue exploring the dataset.\n\n# check the variable names\nnames(df)\n\n[1] \"president\"     \"start_date\"    \"end_date\"      \"approving\"    \n[5] \"disapproving\"  \"unsure_nodata\"\n\n# see the names of Presidents and data sources \ntable(df$president)\n\n\nBarack Obama Bill Clinton Donald Trump  George Bush    Joe Biden       W Bush \n         418          216          141          113           44          282 \n\n\nLet’s see the summary of all variables.\n\n# summary of all variables\nsummary(df)\n\n  president          start_date          end_date           approving    \n Length:1214        Length:1214        Length:1214        Min.   :25.00  \n Class :character   Class :character   Class :character   1st Qu.:42.00  \n Mode  :character   Mode  :character   Mode  :character   Median :48.00  \n                                                          Mean   :50.28  \n                                                          3rd Qu.:57.00  \n                                                          Max.   :90.00  \n  disapproving  unsure_nodata   \n Min.   : 6.0   Min.   : 1.000  \n 1st Qu.:36.0   1st Qu.: 4.000  \n Median :46.0   Median : 6.000  \n Mean   :43.6   Mean   : 6.124  \n 3rd Qu.:52.0   3rd Qu.: 8.000  \n Max.   :71.0   Max.   :43.000  \n\n\nNote that it is pooled: it is shown for all Presidents in the data. For example, let’s see the five-point summary for approval rate.\n\n# summary of the approving\nsummary(df$approving)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   42.00   48.00   50.28   57.00   90.00 \n\n\nThis is the summary not for a specific President, but for all Presidents in the dataset. It is called pooled because data for all presidents are ‘pooled’ together for analysis regardless of who they are.\nIt is a good way to check the data for inconsistencies but it is not really good for analysis because it is giving me the descriptive summary for all presidents at once. I want to break it down by President.\nStill, let’s continue exploring inconsistencies. For example, data for approval, disapproval, and unsure ratings cannot be more than 100 or less than 0. They need to add up to roughly 100.\n\n# summary of the approving\nsummary(df$approving)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  25.00   42.00   48.00   50.28   57.00   90.00 \n\n# summary of the disapproving\nsummary(df$disapproving)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    6.0    36.0    46.0    43.6    52.0    71.0 \n\n# summary of unsure\nsummary(df$unsure_nodata)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   4.000   6.000   6.124   8.000  43.000 \n\n\n\n# are there any inconsistent values? \n# add them up:\ntable(df$approving + df$disapproving + df$unsure_nodata)\n\n\n  99  100  101 \n   2 1209    3 \n\n\nA few of summations add up to 99 and 101, but these are most likely because of rounding. Data looks good and reliable.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Libraries and Operators</span>"
    ]
  },
  {
    "objectID": "04_pipe.html#five-point-summary-for-each-us-president",
    "href": "04_pipe.html#five-point-summary-for-each-us-president",
    "title": "4  Functions, Libraries and Operators",
    "section": "4.5 Five point summary for each US President",
    "text": "4.5 Five point summary for each US President\nNext, we want to create a five point summary for each US President. Let’s say we want to compare their means, medians, and standard deviations.\nI want to have the table below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npresident\napprove_n\napprove_min\napprove_q1\napprove_q2\napprove_q3\napprove_max\napprove_mean\napprove_sd\n\n\n\n\nBarack Obama\n418\n40\n45\n47.0\n50.75\n67\n47.97\n5.37\n\n\nBill Clinton\n216\n37\n50\n57.0\n60.25\n73\n55.49\n7.62\n\n\nDonald Trump\n141\n34\n38\n40.0\n42.00\n49\n40.38\n3.12\n\n\nGeorge Bush\n113\n29\n50\n65.0\n73.00\n89\n61.42\n15.01\n\n\nJoe Biden\n44\n36\n39\n41.0\n43.00\n57\n42.80\n5.84\n\n\nW Bush\n282\n25\n37\n50.5\n62.00\n90\n51.35\n15.87\n\n\n\n\n\n\n4.5.1 Dealing with grouped data using tidyverse\nTidyverse is the name of a set of packages developed by people behind R Studio. It is commonly used for data wrangling, data analysis, and visualisation. They simplify base R by bringing fast, easy-to-use and useful functions. You can check their website: https://www.tidyverse.org/\nThey are especially useful for grouped data. We are going to use group_by and summarize functions to calculate descriptive statistics by US President.\nTidyverse should be installed on UEA computers in the IT Lab. If you are using your own laptop, you need to install it once.\n\n# using your own laptop: install tidyverse once\ninstall.packages(\"tidyverse\")\n\nAfter installation, you can call the library.\n\nlibrary(tidyverse)\n\nTime to group by President. I am going to use the pipe operator to make writing easier. Let’s calculate the mean of approval rating for each US President.\n\n# simplest way to find the mean for grouped data\ndf |&gt; group_by(president) |&gt;  summarize(mean(approving))\n\n# A tibble: 6 × 2\n  president    `mean(approving)`\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Barack Obama              48.0\n2 Bill Clinton              55.5\n3 Donald Trump              40.4\n4 George Bush               61.4\n5 Joe Biden                 42.8\n6 W Bush                    51.3\n\n\n\n\n\n\n\n\nNote\n\n\n\nsummarize() is a tidyverse function. summary() is a base R function. Two functions are different.\n\n\nThis is a good start, but we can do better than this. mean(approving) looks awkward. Let’s rename this. Also, we can make our code more readable.\n\n# a nicer version for finding the mean for each presidents\ndf |&gt; \n  group_by(president) |&gt;  \n  summarize(approval_mean = mean(approving))\n\n# A tibble: 6 × 2\n  president    approval_mean\n  &lt;chr&gt;                &lt;dbl&gt;\n1 Barack Obama          48.0\n2 Bill Clinton          55.5\n3 Donald Trump          40.4\n4 George Bush           61.4\n5 Joe Biden             42.8\n6 W Bush                51.3\n\n\nThis is an excellent start! We can see that George W Bush has the highest mean approval rating whereas Donald Trump has the lowest mean approval rating. What about the highest and lowest approval ratings ever? These correspond to minimum and maximum. We can also calculate these.\n\n# mean, max and min\ndf |&gt; \n  group_by(president) |&gt;  \n  summarize(\n    approval_mean = mean(approving),\n    approval_min  = min(approving),\n    approval_max  = max(approving)\n  )\n\n# A tibble: 6 × 4\n  president    approval_mean approval_min approval_max\n  &lt;chr&gt;                &lt;dbl&gt;        &lt;int&gt;        &lt;int&gt;\n1 Barack Obama          48.0           40           67\n2 Bill Clinton          55.5           37           73\n3 Donald Trump          40.4           34           49\n4 George Bush           61.4           29           89\n5 Joe Biden             42.8           36           57\n6 W Bush                51.3           25           90\n\n\nGeorge W Bush has both the maximum and minimum approval ratings. If you know the early 21st century US history, this is most likely because of 9/11 attacks (see rally ’round the flag effect) and the failures of Iraq War and the 2008 Financial Crises (see Great Recession).\nWe can also see less obvious statistics, such as highest minimum (Barack Obama with 40) and lowest maximum (Donald Trump with 49).\nRecall that for a full descriptive summary, we need to show:\n\nn (the number of observations)\nminimum\nfirst quartile\nmedian\nsecond quartile\nmaximum\nmean\nstandard deviation\n\nNow that we have a good grip of the code, we can easily add these features.\n\n# full grouped summary \ndf |&gt; \n  group_by(president) |&gt;\n  summarize(\n    approve_n   = length(approving),\n    approve_min = min(approving),\n    approve_q1  = quantile(approving, probs = 0.25),\n    approve_q2  = quantile(approving, probs = 0.50),\n    approve_q3  = quantile(approving, probs = 0.75),\n    approve_max = max(approving),\n    approve_mean = mean(approving),\n    approve_sd = sd(approving),\n    )\n\n# A tibble: 6 × 9\n  president   approve_n approve_min approve_q1 approve_q2 approve_q3 approve_max\n  &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;       &lt;int&gt;\n1 Barack Oba…       418          40         45       47         50.8          67\n2 Bill Clint…       216          37         50       57         60.2          73\n3 Donald Tru…       141          34         38       40         42            49\n4 George Bush       113          29         50       65         73            89\n5 Joe Biden          44          36         39       41         43            57\n6 W Bush            282          25         37       50.5       62            90\n# ℹ 2 more variables: approve_mean &lt;dbl&gt;, approve_sd &lt;dbl&gt;\n\n\nYou can take this summary into a different object. Let’s call this sum_app (or any other name of your choice).\n\nsum_app &lt;- df |&gt; \n  group_by(president) |&gt;\n  summarize(\n    n   = length(approving),\n    min = min(approving),\n    q1  = quantile(approving, probs = 0.25),\n    median  = quantile(approving, probs = 0.50),\n    q3  = quantile(approving, probs = 0.75),\n    max = max(approving),\n    mean = mean(approving),\n    sd = sd(approving),\n    )\n\nNow, that it is in an object, we can write it into a csv file and use it in our report.\n\n# write approval summary into a file:\nwrite.csv(sum_app, \"data/approval_summary_bypres.csv\", row.names = F)\n\nAfter doing this, I realized that mean and standard deviation has too many decimal points! What do I do? Just, wrap a round() before mean and sd.\n\n# it is enough to have two decimal points for mean and sd\nsum_app$mean &lt;- round(sum_app$mean, 2)\nsum_app$sd &lt;- round(sum_app$sd, 2)\n\n# write it again\n# write approval summary into a file:\nwrite.csv(sum_app, \"data/approval_summary_bypres.csv\", row.names = F)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Functions, Libraries and Operators</span>"
    ]
  },
  {
    "objectID": "05_reading.html",
    "href": "05_reading.html",
    "title": "5  How to read a research article",
    "section": "",
    "text": "Demonstration: Wiedemann 2024\nWe look into the Electoral Consequences of Household Indebtedness under Austerity by Andreas Wiedemann. You can download the article here: https://doi.org/10.1111/ajps.12708. It should be open access, but in case it is not, you can gain access through your institution by using your University credentials.\nWe will go over the article in detail, starting with the abstract.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#abstract",
    "href": "05_reading.html#abstract",
    "title": "5  How to read a research article",
    "section": "5.1 Abstract",
    "text": "5.1 Abstract\nThe article starts with a general question:\n\nWhat are the political consequences of rising household debt in the context of fiscal austerity?\n\nThis is a clearly formulated question. From this question, we can guess the main explanatory variable, the rising household debt. After reading the first sentence, I have a very good idea that this article will study the impact of the rising household debt.\nThe impact on what, though? As I mentioned, this opening question is general, so the outcome variable is still a little vague. The article will look into the political consequences, but this can be anything. We need to be patient and read more.\nLet’s look at the next two sentences:\n\nI argue that cuts in welfare benefits privatize social obligations as voters address ensuing financial shortfalls by borrowing money. Debt recommodifies individuals and shifts their electoral support from incumbents to opposition and anti-establishment parties by provoking feelings of political neglect, economic vulnerability, and strong emotional responses.\n\nThe theoretical argument is fully laid down here. It is a dense statement, but the theoretical mechanisms are formulated clearly. Let’s try to sketch the theoretical framework using a flowchart (see Figure 5.2).\n\n\n\n\n\n\n\nflowchart TD\n    A[Welfare cuts] --&gt; |Borrowing money\\nto address financial\\nshortfalls| C[Indebtedness]\n    C --&gt; |Recommodification\\nof the individual| D[Emotional response:\\nFeelings of\\npolitical neglect and\\neconomic vulnerability]\n    D --&gt; |Change in\\npolitical behaviour| E[Shifts in electoral support\\nfrom incumbents to opposition\\nand anti-establishment parties]\n\n\n\n\nFigure 5.2: Theoretical framework formulated in the article\n\n\n\n\n\n\nThis is the theoretical framework of the article! We also have a very good idea of the outcome variable now: vote choice. Or, more precisely, the vote share of the incumbent, opposition and anti-establishment parties.\nThe article expects to find a relationship between indebtedness and support for opposition/anti-establishment parties.\nLet’s continue:\n\nI examine this argument by leveraging spatial and temporal variation in the rollout of Universal Credit (UC), a large-scale welfare reform in the United Kingdom. Using fine-grained administrative data on unsecured debt, I demonstrate that fiscal austerity generated an increase in indebtedness, which lowered support for the incumbent Conservatives and strengthened support for Labour and the UK Independence Party (UKIP).\n\nHere, the article is summarizing the empirical analysis. It tests its arguments using data from the UK. We now have a good idea on the empirical analysis and measurement.\nLet’s unpack. Consider the theoretical concepts, and corresponding measurement. How does the article measure these concepts?\nConcept: measurement\n\nCuts in welfare benefits: variation in the rollout of Universal Credit (UC)\nIncreasing debt recommodifies individuals: fine-grained administrative data on unsecured debt (increase in indebtedness)\nfeelings of political neglect, etc.: ? [still unclear]\nshifts in electoral support from the incumbent to opposition and anti-establishment parties: support for the incumbent Conservatives and support for Labour and the UK Independence Party (UKIP).\n\nWe have moved from abstract concepts to tangible measurement. Yet, #3 is still undefined. Let’s continue.\n\nI then use individual-level survey data to explore the mechanisms that link debt and political behavior. The results suggest that rising indebtedness increases the political costs of welfare retrenchment and creates new political cleavages.\n\nOk, now the article tells me that there is a second study. This is a survey and it is at the individual-level. Is it possible that this is how it is going to capture the feelings of individuals? Based on the theoretical framework, the mechanisms that link debt and political behavior must refer to feelings of political neglect.\nIf we re-read the previous sentences, we can also capture that there are at least two main studies, one is at the administrative level, and the other one is at the individual-level.\nBy just reading the abstract, we have learned a lot about this article! Some parts are still unclear, but we have a very good idea of what to expect.\nWe can also understand that this is an observational study. The empirical analysis is based on observational data. There is no mention of an experiment.\n\n\n\n\n\n\nTip: Skim the article first\n\n\n\nBefore reading any further, I would quickly skim the whole article. Focus on the section titles, organization, figures and tables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#introduction",
    "href": "05_reading.html#introduction",
    "title": "5  How to read a research article",
    "section": "5.2 Introduction",
    "text": "5.2 Introduction\nThe first three paragraphs quickly get us into the heart of the question: the political consequences of austerity. These paragraphs also engage with the literature and present a research gap.\nIn the fourth paragraph, the article introduces its contribution.\n\nThis article brings a different perspective to this literature by focusing on individuals’ responses to cuts in public spending as a key factor that shapes political behavior.\n\nThe focus is on the underlying mechanisms that link austerity/welfare cuts and political behaviour:\n\nDespite the potential gravity of rising household debt, we have little systematic knowledge of the political consequences of borrowing money to fill financial gaps and the underlying mechanisms of these effects.\n\nAfterwards, this paragraph presents a concise summary of the theoretical framework.\n\nI argue that … [THEORETICAL FRAMEWORK SUMMARIZED]\n\nIn the fifth paragraph, starting with “I examine my argument …”, the article turns to testing the implications of the theoretical argument empirically. A brief summary of the case the article is going to analyze is presented: the UK, introduction of the Universal Credit, and increase in the household debt.\nLinking the cuts in welfare (i.e., the rollout of the Universal Credit) and the increase in household debt is an important component of the article. This is a key causal chain of the theory:\n\n\n\n\n\n\nflowchart LR\n    A[Welfare cuts] --&gt; B(Borrowing money\\nto address financial\\nshortfalls) --&gt; C[Indebtedness]\n\n\n\n\n\n\n\nThe article tries to establish this link empirically:\n\nAs I will document, households respond to cuts in social benefits by borrowing money to address financial shortfalls and pay for essential goods and services.\n\nNext two paragraphs unpack the empirical strategy. Starting with:\n\nI provide three types of empirical evidence to support the links between social spending cuts, indebtedness, and electoral behavior.\n\nThis is a nice placeholder. The reader now knows what to expect. There are three steps in the causal chain. I expect at least three pieces of analysis to test each mechanism.\n\nFirst, I leverage spatial and temporal variation in the rollout of UC, combined with novel administrative data on personal unsecured loans at the postcode sector level, to estimate the effect of welfare retrenchment on household indebtedness.\n\nThis sentence summarizes the first study analyzing the relationship between austerity and household indebtedness. It also tells that this part is at the postcode sector level. It is akin to a study at the neighbourhood level. There is some aggregation. I expect that you cannot access individual-level data on debt (because of privacy reasons), but data is available on an aggregated level.\nLooking into variations in \\(X\\) plays a key role in analyzing changes in \\(Y\\). In this respect, the article looks at the different pace with which Universal Credit was introduced in different localities. This is the variation in \\(X\\) (for the first study). The article also looks at the variation in \\(Y\\) (for the first study), referring to different levels of indebtedness in a locality. Is there a relationship between shifts to the less generous Universal Credit scheme and indebtedness?\n\n\n\n\n\n\nflowchart LR\n  A[Shift to Universal Credit] --&gt; B[Indebtedness]\n\n\n\n\n\n\n\nThe following sentences in this paragraph are a little bit too detailed for our purposes. Let’s say that we didn’t understand them, but we don’t need to get stuck on this because we acquired the main message of the article here:\n\nI show that one additional UC benefit recipient in a postcode sector increases unsecured debt by an average of £665.\n\nThis sentence conveys the message that the article empirically links welfare cuts and the level of indebtedness.\nLet’s continue with the next paragraph.\n\nSecond, I study how indebtedness affects constituency-level electoral behavior in the 2015, 2017, and 2019 UK general elections.\n\nNow, we are in the second mechanism in the causal chain.\n\nAusterity-induced indebtedness, measured by the variation in unsecured debt induced by UC rollout, increases political participation, lowers vote shares for the incumbent Tories and the Liberal Democrats, and strengthens support for the opposition Labour party and UKIP.\n\nAs we emphasized, looking into variations in \\(X\\) plays a key role in analyzing changes in \\(Y\\). In this respect, the article studies whether there is a relationship between indebtedness and vote share. Note that this is the second study, where the explanatory variable is indebtedness and the outcome variable is the vote share of the incumbent.\n\n\n\n\n\n\nflowchart LR\n  A[Indebtedness] --&gt; B[Vote Share]\n\n\n\n\n\n\n\nFollowing sentence:\n\nI substantiate these results with individual-level data from the British Election Study (BES).\n\nThis is an unexpected addition: there is a second study to strengthen the second mechanism. This can be thought of as ‘study 2(b)’. It is at the individual-level, meaning that it is based on responses of individuals to the BES survey. Do we see a link between indebtedness and vote-choice in the BES data? The article looks into this and reports that it establishes a relationship:\n\nRespondents who report that they have taken out loans to pay for essential goods and services– which indicates borrowing during times of financial distress – are more likely to turn out to vote and punish the Tories and cast their ballots at higher rates for Labour and UKIP.\n\nThe first paragraph of page 356 discusses the third piece of empirical evidence:\n\nFinally, I examine several mechanisms that help explain how rising indebtedness caused by fiscal austerity measures affects political behavior. Using survey data from the BES, I show that debtors – particularly those who borrow money for essential goods and services – are more worried about their economic security, demand a stronger social safety net, and feel unequally treated and politically neglected. Debtors feel particularly angry and resentful toward the Conservative party and more hopeful toward Labour.\n\nThis final set of empirical evidence is about establishing a link between indebtedness and emotional response. Recall that this is a crucial step in the causal chain:\n\n\n\n\n\n\nflowchart LR\n  A[Indebtedness] --&gt; B[Emotional Response]\n\n\n\n\n\n\n\nIn the final paragraph of the introduction, the article highlights its contributions and summarizes its findings and conclusions. Let’s look at the final sentence:\n\nThe article’s findings suggest that voters are not necessarily loyal partisans but respond to policy changes and form electoral choices based on their personal interests. Debt is an increasingly salient factor that influences political behavior.\n\nLet’s end with three important warnings!\n\n\n\n\n\n\n1. Introduction is probably the most important part\n\n\n\nAll you need to learn about this article is concisely presented in the introduction.\nThis is not fictional writing: there are no plot twists and no hidden secrets saved for later.\nAll the main parts are laid out here. Introduction is a mini version of the whole article!\n\n\n\n\n\n\n\n\n2. Pay attention to citations\n\n\n\nLook at how the article engages with the literature and how it cites sources. Check the bibliography.\nI encourage using this style for citations and the bibliography. They are called in-text citations. It is a brief way to cite, without wasting too much of word-count. The format is clear: (Author, Year).\nThere are no long ‘filler’ sentences masked as citations. Sometimes students write long filler sentences that are clearly aimed at reaching a target word count. A hypothetical example:\n\nCharles Robert Darwin, who was an influential naturalist and biologist, published a seminal book through the John Murray publishing house on 25 November 1859, titled ‘On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life’, which introduced the theory of evolution to a larger audience.\n\nYou will lose points if you write like this. Instead, convey your message clearly:\n\nDarwin (1859) introduced the theory of evolution.\n\nYou can use this article as a guideline for citation rules.\nAlso, look at the end of the first paragraph (on the first page). Newspaper sources are cited via a footnote. This is because newspapers are different from scholarly sources. Using (Author, Year) would be awkward, so the article switches to footnote for citing these newspaper sources. This is perfectly fine for popular/non-academic sources (newspaper, website, twitter/social media, etc.) and makes the text more accessible.\n\n\n\n\n\n\n\n\n3. Style and organization are important\n\n\n\nPay close attention to the writing style. This style is common in academic and professional writing.\nThe text is well-written and well-organized. It is easy to understand and navigate. There are no extremely long or convoluted sentences. The text is organized under sections and subheadings. A well-organized text helps the author guide the reader toward the core points and convey the message as intended.\nThe writing style is formal but accessible. The tone is professional, but the author does not shy away from using ‘I’. First-person writing is fine. Let’s see a sentence from the article:\n\nI argue that cuts in welfare benefits privatize social obligations as voters address ensuing financial shortfalls by borrowing money.\n\nThis sentence clearly conveys the message of the article. It is excellent.\nThis is first-person writing, but not personal writing. What do I mean by personal writing? Let’s see an example:\n\nI have always been interested in welfare cuts and their impact on political behaviour as I lived through the austerity measures.\n\nThis sentence does not tell me much. It just insinuates a personal connection without any substance.\nPersonal writing is fine in many contexts, including some professional contexts such as a cover letter, but it might not be the best approach in an empirical research paper (unless you are using the method on purpose, as in some qualitative methods).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#literature-review-and-theory",
    "href": "05_reading.html#literature-review-and-theory",
    "title": "5  How to read a research article",
    "section": "5.3 Literature Review and Theory",
    "text": "5.3 Literature Review and Theory\nThis article does not have a designated ‘literature review’ section, but it does melt its review of the literature into multiple sections, most notably the second section.\nThe second section, Privatizing Social Obligations: The Electoral Consequences of Indebtedness, both reviews the literature and fully presents the theoretical framework. We already had a very good idea of the theory, but the details are further explained here.\nThe opening sentence is a general question. Recall the lecture where we discussed research questions. I emphasized that we usually start with a general, larger question and slowly narrow it down to a more specific question. A similar approach is adopted here to introduce the main question:\n\nWhy people turn out to vote and why they decide to punish or reward incumbents at the ballot box are important questions for effective governance and democratic accountability.\n\nA concise but informative review of the literature follows this sentence. In the next paragraph, the article turns to its proposed explanation. It highlights the differences in its approach:\n\nThis article offers a different perspective on these questions to cuts in public spending as a critical factor that shapes political behavior more generally. Prior work largely overlooks citizens’ responses to cuts in public spending as a critical factor that shapes political behavior more generally.\n\nStarting with the next paragraph, the article presents its theoretical framework: “I argue that the rise …”. In this paragraph, I would like to highlight one sentence that captures the core of the argument:\n\nThis individualization and privatization of social obligations turns some voters into debtors and undermines electoral support for incumbents while strengthening voting for opposition parties, including the far-right.\n\nPay attention to such key sentences. They tend to come in the first two sentences of a paragraph.\nThe next paragraph (the first one on page 357) tells that this privatization of social obligations “has several reinforcing political consequences.”” Several is an important word here, because it signals that an enumeration of causal mechanisms will follow. Indeed, the next sentence starts with “First”. Such sentences are important markers:\n\nFirst, it changes people’s perception of the role of government and fuels their sense of political neglect and unequal treatment as they go into debt.\n\n\nSecond, debt makes salient the inadequacy of the welfare state and declining social support.\n\n\nFinally, by increasing feelings of political neglect, status threat, and economic insecurity, the privatization of social obligations triggers emotional responses among people who go into debt to make ends meet. Anger is a particularly powerful emotion.\n\nThe theoretical framework is fully laid out. The article now turns back to engaging with previous research because its main argument runs contrary to some established findings in the literature. The paper clearly acknowledges this and tackles it head-on, starting with:\n\nResearch has shown that individuals who experienced adverse life events and economic crises are less likely to vote (Bartels 2013; Ojeda, Michener, and Haselswerdt 2020). I argue, however, that borrowing money can increase political participation in two ways.\n\nAlthough there is no designated section for a literature review, it is baked into the article, mostly apparent in the introduction and the theory.\nThe next section takes us to research design and analysis.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#fiscal-austerity-and-indebtedness-in-the-united-kingdom",
    "href": "05_reading.html#fiscal-austerity-and-indebtedness-in-the-united-kingdom",
    "title": "5  How to read a research article",
    "section": "5.4 Fiscal Austerity and Indebtedness in the United Kingdom",
    "text": "5.4 Fiscal Austerity and Indebtedness in the United Kingdom\nThis part explains the case under analysis: the United Kingdom. A short and precise summary is provided. Next, the text focuses on how the Universal Credit was introduced, which created variations across different localities.\n\nThe government introduced UC gradually over several years across different jobcentres in Great Britain.\n\n\nBecause rollout occurred on a jobcentre-by-jobcentre basis, progress varies considerably between postcode areas (Kennedy and Keen 2018). The variation in the rollout of UC over time and across areas offers a unique opportunity to study the consequences of fiscal austerity on indebtedness and, in turn, electoral behavior.\n\nUnder the section “Data and Measurement”, the paper explains its data sources.\nThe first data source is about indebtedness. This source is explained in detail:\n\nthe UK government, the British Bankers Association and the Council for Mortgage Lenders reached an agreement in 2013 that major banks would report quarterly lending data at the postcode sector level to improve transparency about lending locations. A postcode sector has the final two letters of a full postcode removed. These data, published quarterly since 2013, include the level of outstanding personal loans. I obtain these data on the geographic distribution of unsecured personal loans from UK Finance, the trade association for the UK banking and financial services sector. It covers 9292 unique postcode sectors. I aggregate postcode sectors into four-digit postcode districts, the lowest level of geography for which data on UC beneficiaries is available,\n\nThe main outcome variable is the vote choice. This paper studies UK general elections (2015, 2017, 2019):\n\nParliamentary constituency–level results for the 2015, 2017, and 2019 UK general elections come from Pippa Norris’s British Parliamentary Constituency Database.\n\nHow to measure welfare cuts? They are measured by looking at the differences between the Universal Credit rollout. The Department for Work and Pensions (DWP) is the source of this data:\n\nData on UC rollout come from the DWP, which collects information on the number of people on UC at the postcode district and parliamentary constituency levels.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#analysis",
    "href": "05_reading.html#analysis",
    "title": "5  How to read a research article",
    "section": "5.5 Analysis",
    "text": "5.5 Analysis\nThe analysis starts with the section ‘Descriptive Patterns’. Here, the article provides descriptive statistics and summaries of the data. Note that numerical and visual summaries are presented.\nLook at the tables and figures. Pay attention to the presentation. They are numbered, and they have titles and captions. This style is important.\nStudying the relationship between variables of interest starts with the section “Welfare State Retrenchment and Indebtedness”. A regression model to “estimate the effect of the UC rollout on unsecured debt” is presented.\nIn this part of the analysis, \\(UC_{it}\\) is the explanatory variable:\n\n\\(UC_{it}\\) is the number of UC recipients in postcode district i and quarter-year t.\n\nThe outcome variable, \\(Y_{it}\\), is the log of the average unsecured household debt, where \\(i\\) refers to locality (postcode district level) and \\(t\\) refers to time (quarter-year).\nWhat is a log transformation? It refers to a logarithm (in case you want to check more: https://en.wikipedia.org/wiki/Logarithm). The details are not relevant for us, but we can capture the essence. Taking the log of a number makes it smaller.\nLet’s see an example:\n\n# some numbers:\nnums  &lt;- c(10, 100, 1000, 10000)\n\nnums\n\n[1]    10   100  1000 10000\n\n# let's see their logs (base 10):\nlog(nums, base = 10)\n\n[1] 1 2 3 4\n\n\nWhy are we doing this? Recall our discussion on income/wealth. There can be extremely wealthy individuals. When the data is spread over a wide spectrum with extremely high/low values (i.e., the data is skewed), we might want to address this. Here, taking the logarithm of a variable comes in handy.\nImagine that there are three people: Alice, Bob, and Eve. Their net worths are, respectively, £10 thousand, £1 billion, and £10 billion. If we do not do any transformation, the respective wealths of Alice and Bob are closer to each other than that of Bob and Eve. The difference between Alice and Bob’s wealth is £999 million 990 thousand, while the difference between Bob and Eve’s wealth is £9 billion. So, can we say that Alice and Bob are similar and Eve is super-rich? Not really. In reality, Bob and Eve are super-rich billionaires; they are more alike. Let’s take their logarithms.\n\n# wealth of three individuals \nwealth &lt;- c(\n  10^5,  # 10 thousand\n  10^12, # 1 billion\n  10^13 # 10 billion\n  )\n\n# take the logarithm\nlog(wealth)\n\n[1] 11.51293 27.63102 29.93361\n\n\nAfter taking the logarithm, the billionaires (Bob and Eve) are closer to each other with ‘27.63’ and ‘29.93’ units of wealth, respectively.\nTaking the logarithm of a variable can also make relationships look more linear. Let’s see an example.\n\n\n\n\n\n\n\n\n\nLet’s continue reading the article. We don’t need to understand all the details; we are trying to achieve a general understanding.\nA regression analysis is carried out. We are not going to get stuck in the technical details. The technique employed here is called a “difference in differences”, which we have not covered in the module (textbooks assigned as further readings have chapters on this technique). In simple terms, it compares the average change in indebtedness in localities where Universal Credit is not introduced yet with the average change in indebtedness in localities where Universal Credit is introduced.\nFor example, imagine two localities: Norwich and York. In time \\(t\\), both Norwich and York do not have Universal Credit. In time \\(t + 1\\), Norwich goes into Universal Credit whereas it has not been introduced in York yet. We calculate the difference in indebtedness in Norwich before and after Universal Credit. This is our first difference. We also calculate how indebtedness changed in York between time \\(t\\) and time \\(t + 1\\). This is our second difference. Finally, we compare these two differences. This is why it is called ‘difference in differences’. Maybe indebtedness increased in both places, but if it is increased more in Norwich than York, could it be possible that it is due to Universal Credit?\nI want to highlight one sentence here, on page 361:\n\nThe identifying assumption is that no time-varying confounders affect the relationship between UC rollout and debt, such that changes in the number UC recipients are exogenous to changes in unsecured debt over time.\n\nThis sentence is a little bit hard to crack, but it essentially states that there are no confounding variables (\\(Z\\)). The reverse of exogenous is endogenous, which refers to confounders where a \\(Z\\) variable affects both \\(X\\) and \\(Y\\). In this regard, the article argues that there is no such \\(Z\\) (see Figure 5.3).\n\n\n\n\n\n\n\ngraph TD\n    Z --&gt; X(X: UC rollout)\n    Z --&gt; Y(Y: Debt)\n\n\n\n\nFigure 5.3: The article claims that there is no such Z variable exists. Presence of such a confounder (Z) means endogeneity.\n\n\n\n\n\n\nIn simple terms, the reason behind the different pace of implementing Universal Credit in different localities (\\(Z\\)) is no way connected to debt (\\(Y\\)), then we can consider that higher increase of indebtedness in Norwich than York is due to the introduction of Universal Credit.\nObviously, we don’t do it only for our imagined example of Norwich and York, but for all localities and look at the average change.\nNext paragraph summarizes the results from a regression analysis:\n\nThe results in Table 2 show that UC rollout significantly increased the amount of unsecured debt in a given postcode district. […] unsecured debt increases by about £685 for every additional person receiving UC. With average weekly gross pay in Great Britain of about £511, this is a sizeable effect.\n\nThis section continues with some additional analysis to see how robust this result is to alternative modelling strategies. However, the main result stays intact: the indebtedness in a locality increases following the introduction of Universal Credit.\n\nElectoral Consequences of Indebtedness under Fiscal Austerity\nThe first sentence of this section is a restatement to remind the reader what the argument is. Such repetitions and restatements are necessary for organizing the text:\n\nFiscal austerity privatizes social obligations and recommodifies individuals through debt. As a result, I argue that borrowers shift their electoral support from incumbents to the opposition because they feel politically neglected, economically vulnerable, and lacking of social support.\n\nThe following sentence tells us what to expect next:\n\nThis section examines how indebtedness affects political behavior with two complementary empirical strategies before presenting evidence on the underlying mechanisms in the next section.\n\nRecall that the analysis has three components:\n\nUniversal Credit \\(\\rightarrow\\) Unsecured Debt\nUnsecured Debt \\(\\rightarrow\\) Political Behaviour\n\nConstituency-level (data: election results)\nIndividual-level (data: the BES survey)\n\nUnsecured Debt \\(\\rightarrow\\) Emotions (this is the mechanism through which unsecured debt influence political behaviour)\n\nThe article already presented the first analysis, and is turning to the second one, which has two sub-components.\n\n(a) Constituency-Level Results\nAgain, we are not going to get stuck in technical details and march toward the main result. Starting with the analysis 2.a, the article explains that the analysis is at the constituency-level:\n\nI study the effect of austerity-induced indebtedness on turnout and party vote shares in the 2015, 2017 and 2019 general elections\n\nA regression analysis is carried out. Again, we are not going to get stuck in the technical details. Let’s try to clarify the outcome and explanatory variables (\\(X\\) and \\(Y\\)). The statement quoted above gives this information. The independent variable is the austerity-induced indebtedness. Note that this is not just indebtedness, austerity-induced indebtedness. This is estimated through the first study. When the article discusses two-stage regression, the first stage refers to estimating austerity-induced indebtedness.\nFor the outcome, the study analyze both the turnout and party vote share. In this regard, this stage has multiple dependent variables: the article will analyze the turnout and the party vote share of major parties (i.e., Tories, LibDem, Labour, UKIP).\nIn addition to outcome and independent variables, the article also discusses control variables. These are possible ‘confounders’ that may influence the variables of interest. Most importantly, we want to keep our units as similar as possible for meaningful comparison. See the following passage (on page 363):\n\n\\(X_{it}\\) is a matrix of constituency-level covariates, including unemployment rates, mean annual gross pay for full-time workers, the share of individuals with a bachelor’s degree or higher, and the share of people 60 years and older. I further control for median house prices, which have been found to be associated with lower turnout (McCartney 2021) and voting for populist parties (Adler and Ansell 2020). House prices also serve as a proxy for local economic conditions, because a strong local economy can increase incumbents’ chances of re-election (Larsen et al. 2019). Homeowners are more likely to hold conservative views and less likely to support redistribution when house prices are rising (Ansell 2014).\n\nThe paragraph starting with “Figure 3 show the results” (on page 363) discusses the results. Look at Figure 3 and read this paragraph carefully. The last sentence of this paragraph contextualize the results:\n\nA 10% increase in debt strengthens turnout by about 0.52 percentage points and reduces the vote shares for the Tories and Liberal Democrats by 0.69 and 0.96 percentage points, respectively. By contrast, the opposition parties gain electorally as austerity-induced debt rises. For a 10% increase in debt, the Labour party and UKIP gain 2.45 and 0.68 percentage points, respectively.\n\nTo summarize the constituency level findings, the turnout is higher in constituencies with higher levels of unsecured debt (induced by Universal Credit) than in constituencies with lower levels of unsecured debt. The respective vote shares of Tories and Liberal Democrats are lower in constituencies with higher levels of unsecured debt. Conversely, the vote shares of the opposition Labour and UKIP are higher in constituencies with higher levels of unsecured debt.\n\n\n(b) Micro-Level Results\nThe article continues with what we have termed ‘study 2.b’, which refers to individual-level analysis. This section estimates an individual’s probability of voting (for a particular party). In simple terms, we are interested in the differences between those who have unsecured debt and those who don’t in terms of their voting behaviour. Do debtors behave differently from non-debtors when it comes to voting?\nAs we discussed in the lectures, we want to make these groups (debtors and non-debtors) as similar as possible except for their indebtedness, and compare their political behaviour. However, these two groups can be quite different in other aspects. The author acknowledges this problem and explains the steps they have taken to address it, namely considering other observable factors in the analysis:\n\n\\(X_i\\) is a matrix of socioeconomic covariates that could confound the relationship between indebtedness and political behavior. I include age, education, income, number of children in the household, and dummy variables for gender, marital status, unemployed, student, retired, homeowner status, and union membership. I further control for respondents’ class status, which traditionally occupied a central position in British voting patterns.\n\nMain results are discussed on page 364 and shown in Figure 4:\n\nThe left panel of Figure 4 shows that respondents who borrow money for essentials are 1.6 times more likely to turn out and vote, compared to those who do not borrow for essentials. They are considerably less likely to vote for the incumbent Tories (a 50% probability) and 1.5 times more likely to support Labour and 1.8 times more likely to support UKIP.\n\nOne more thing to highlight here: the article makes a distinction between ‘borrowing for essentials’ and more general borrowing (i.e., ‘unsecured debt’). ‘Borrowing for essentials’ is a subset of ‘unsecured debt’, but it is more relevant for the purposes of this study because those who had to borrow for essentials such as food are more likely to be hit by welfare cuts than those who borrowed to buy, for example, luxury consumption goods. The author argues that although the results are not that strong for general borrowing, there is a strong relationship between ‘borrowing for essentials’ and voting behaviour, which is exactly where the theoretical framework expects to find a relationship.\nThe article then turns to some other questions in the BES survey related to the welfare state, such as respondents’ views on cuts to local services and public spending.\n\n\n\nInvestigating the Mechanisms\nIn the penultimate section, titled Investigating the Mechanisms, the article presents the third component of the empirical analysis, which tackles the relationship between unsecured debt and emotional response. Recall that emotional response is a crucial mid-step in the causal chain (see Figure 5.2).\nApply the approach we have used so far to read the text. Do not get stuck in technical details and try to cut to the core of the argument and results. Focus on Figures 5, 6, and 7. Try to understand them as they visually summarize the results.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "05_reading.html#discussion-and-conclusion",
    "href": "05_reading.html#discussion-and-conclusion",
    "title": "5  How to read a research article",
    "section": "5.6 Discussion and Conclusion",
    "text": "5.6 Discussion and Conclusion\nThis section begins with a concise summary of the article, highlighting the main arguments and findings. It is short and accessible, similar to an ‘executive summary’: readers who don’t go through the entire text can still grasp the key points by reading this first paragraph. It is where the author presents the most important points to take away. In this regard, there are similarities between the introduction and conclusion sections, but the conclusion is usually the shortest section of the text (besides the abstract). The conclusion section needs to be concise. It should be dense but accessible.\n\n\n\n\n\n\nDo not introduce a new argument in conclusion\n\n\n\nThe conclusion section is there to summarize and highlight the most important points raised in the article. You should not use this section to go into deep, substantive discussions by bringing in new evidence and arguments. If such evidence and arguments are essential, they should come earlier in the text. Nothing new should be introduced in the conclusion.\n\n\nThe second paragraph is a concise answer to the question “Why should we care about these findings?”. This is an effective strategy to remind of the reader the importance of the topic at hand. Note that this is not the first time the article emphasizes the important of the topic: this has been done in the introduction as well.\nThe final two paragraphs are about ‘further research’. This is a typical approach in academic writing, but not that common in other sets of professional writing. Discussing the policy implications of the research is another common feature of a conclusion in a scholarly piece.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>How to read a research article</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html",
    "href": "06_crosstab.html",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "",
    "text": "6.1 Measuring democracy: v2x_polyarchy\nLet’s explore the data with a quick summary().\n# summary of the polyarchy variable \nsummary(df$v2x_polyarchy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0200  0.3400  0.5600  0.5468  0.7800  0.9300\nIn line with the description in the codebook, the values of v2x_polyarchy are between 0 and 1 (more precisely, the minimum is 0.02 and the maximum is 0.93). There are no missing values. It is apparent that v2x_polyarchy is a continuous numerical variable.\nWe can create a histogram to visually summarize the variable.\n# Histogram of polyarchy\nhist(df$v2x_polyarchy,\n     breaks = seq(0, 1, 0.1),\n     main = \"Histogram of V-Dem Polyarchy Index\", \n     xlab = \"Level of democracy (Polyarchy Index)\", \n     ylab = \"Frequency\"\n     )\nWe can also create a boxplot.\n# A box plot\nboxplot(df$v2x_polyarchy, \n        main = \"Distribution of V-Dem Polyarchy Index\", \n        ylab = \"Level of democracy (Polyarchy Index)\",\n        ylim = c(0,1) # y-axis should span from 0 to 1\n        )\nWe should also quickly look at the most and the least democratic countries. We have already learned how to do this in base R, but let’s see another way using tidyverse.\n# Least Democratic (most authoritarian) 10 countries\ndf |&gt; \n  select(Country_Name, v2x_polyarchy) |&gt; # selects only two variables\n  arrange(v2x_polyarchy) |&gt; # arranges (sorts) from lowest to highest\n  head(n = 10) # select only the first 10 rows\n\n   Country_Name v2x_polyarchy\n1    Saudi Arab          0.02\n2       Eritrea          0.07\n3         Libya          0.08\n4         Qatar          0.09\n5         China          0.10\n6       Lao PDR          0.10\n7    Korea, Dem          0.10\n8     Swaziland          0.13\n9          Fiji          0.15\n10      Myanmar          0.15\nFor the most democratic countries, we need to add another tidyverse function ‘desc()’ meaning descending.\n# Most Democratic 10 countries\ndf |&gt; \n  select(Country_Name, v2x_polyarchy) |&gt; # selects only two variables\n  arrange(desc(v2x_polyarchy)) |&gt; # arranges (sorts) from highest to lowest (descending)\n  head(n = 10) # select only the first 10 rows\n\n   Country_Name v2x_polyarchy\n1    United Kin          0.93\n2        Sweden          0.93\n3       Uruguay          0.93\n4     Australia          0.92\n5    Costa Rica          0.92\n6        France          0.92\n7    United Sta          0.92\n8        Norway          0.91\n9    Switzerlan          0.90\n10      Germany          0.90",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html#creating-a-categorical-democracy-variable-from-the-numerical-v2x_polyarchy-variable",
    "href": "06_crosstab.html#creating-a-categorical-democracy-variable-from-the-numerical-v2x_polyarchy-variable",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "6.2 Creating a categorical democracy variable from the numerical v2x_polyarchy variable",
    "text": "6.2 Creating a categorical democracy variable from the numerical v2x_polyarchy variable\nAs we emphasized, v2x_polyarchy is a numerical variable. However, we would also like to conceptualize democracy as a binary variable: a country is either a democracy or not. In this regard, we want to create a categorical (binary) variable from a numerical variable (v2x_polyarchy). Only by doing this, we can calculate the rate of democracies. This will allow us to create a table similar to Table 6.1.\nLet’s decide on a cut-point where any v2x_polyarchy score below this cut-point is considered as an autocracy and anything above is categorized as a democracy. A common cut-point used in the literature is 0.60.\nWe need to create a new variable using this condition:\n\nIf a country’s v2x_polyarchy index is higher than 0.60, then it is a democracy.\n\nIf a country’s v2x_polyarchy index is 0.60 or lower, then it is an autocracy.\n\nLet’s call this new variable regime_type.\n\n# Creating the regime type variable\n# start with an empty vector (of NAs):\ndf$regime_type &lt;- NA\n\n# fill the empty variable regime_type using the appropriate conditions:\ndf$regime_type[df$v2x_polyarchy &gt; 0.60 ] &lt;- \"Democracy\" # v2x_polyarchy is higher than 0.60 \ndf$regime_type[df$v2x_polyarchy &lt;= 0.60 ] &lt;- \"Autocracy\" # v2x_polyarchy is 0.60 or lower\n\nWe created the new variable regime_type, but we should also do some checks for quality control. It is easy to make a mistake while creating a variable. We want to be sure that the new variable is error free.\n\n# Some quality controls \n\n# start with a table:\ntable(df$regime_type, useNA = \"always\")\n\n\nAutocracy Democracy      &lt;NA&gt; \n       95        71         0 \n\n\nThere are 95 autocracies and 71 democracies without any missing data. This looks good. Next, let’s make sure that our cut-point is working well.\nNo country with a v2x_polyarchy index 0.60 or below should be categorized as a democracy. All countries with v2x_polyarchy a score above 0.60 should be democracies. Let’s quickly check:\n\n# make sure to put v2x_polyarchy first, so it goes to rows.\n# regime_type should go to columns (two categories)\n# this way it is easier to see\ntable(df$v2x_polyarchy, df$regime_type)\n\n      \n       Autocracy Democracy\n  0.02         1         0\n  0.07         1         0\n  0.08         1         0\n  0.09         1         0\n  0.1          3         0\n  0.13         1         0\n  0.15         3         0\n  0.16         1         0\n  0.17         3         0\n  0.19         1         0\n  0.2          2         0\n  0.21         1         0\n  0.22         3         0\n  0.23         1         0\n  0.24         4         0\n  0.25         2         0\n  0.27         1         0\n  0.28         1         0\n  0.29         1         0\n  0.3          2         0\n  0.31         2         0\n  0.32         2         0\n  0.33         1         0\n  0.34         5         0\n  0.35         4         0\n  0.36         2         0\n  0.37         2         0\n  0.38         2         0\n  0.42         1         0\n  0.43         4         0\n  0.44         2         0\n  0.45         1         0\n  0.46         2         0\n  0.47         3         0\n  0.48         3         0\n  0.49         2         0\n  0.5          2         0\n  0.51         3         0\n  0.52         2         0\n  0.53         2         0\n  0.55         1         0\n  0.56         2         0\n  0.57         4         0\n  0.58         4         0\n  0.59         1         0\n  0.6          2         0\n  0.62         0         2\n  0.63         0         3\n  0.64         0         1\n  0.65         0         1\n  0.66         0         2\n  0.67         0         2\n  0.68         0         5\n  0.69         0         1\n  0.7          0         4\n  0.71         0         2\n  0.73         0         1\n  0.75         0         1\n  0.76         0         3\n  0.78         0         3\n  0.79         0         3\n  0.81         0         1\n  0.82         0         1\n  0.84         0         1\n  0.85         0         3\n  0.86         0         4\n  0.87         0         2\n  0.88         0         6\n  0.89         0         7\n  0.9          0         4\n  0.91         0         1\n  0.92         0         4\n  0.93         0         3\n\n\n\n\n\n\n\n\nQuality checks are for you\n\n\n\n\n\nData analysis usually involves many steps. We do not report preparatory steps.\nDo not report such steps for quality checks. The table above is informative for us (we are sure that we did it correctly!), but they are not meaningful for the reader!\n\n\n\nIt looks like we did it!\nIf you studied the codebook, you might have realized that there is already a binary democracy variable available. In other words, we did not need to create this new variable, but we did it for the purposes of practice. We can also verify our approach.\n\n# a binary democracy variable was already available in the dataset: \nsummary(df$democracy)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4277  1.0000  1.0000 \n\n# a table:\ntable(df$democracy)\n\n\n 0  1 \n95 71 \n\n# verify that we did it correctly:breaks\ntable(df$democracy, df$regime_type)\n\n   \n    Autocracy Democracy\n  0        95         0\n  1         0        71\n\n\nWe verified the variable we created!\nLet’s visually summarize it using a bar plot.\n\nbarplot(table(df$regime_type), \n        ylab = \"Number of Countries\" \n        )\n\n\n\n\n\n\n\n\n\nFigure 6.1: The number of countries by regime type\n\n\n\n\n\n\n6.2.1 Moving from raw frequencies to percentages\nIn Figure 6.1, we have visualized the number of countries, which refers to raw frequencies. In some instances, we may want to present percentages instead. We can quickly calculate and present them.\nAll we need to do is to use table() function in conjunction with squared-parenthesis notation.\n\n# raw frequencies:\ntable(df$regime_type)\n\n\nAutocracy Democracy \n       95        71 \n\n# the number of democracies\ntable(df$regime_type)[2]\n\nDemocracy \n       71 \n\n# the number of all countries\nsum(table(df$regime_type))\n\n[1] 166\n\n\nThe number of democracies divided by the number of all countries will give use the proportion of democracies.\n\ntable(df$regime_type)[2] / sum(table(df$regime_type))\n\nDemocracy \n0.4277108 \n\n\nThis is the rate of democracies, but there are too many decimal points. Just two decimal points would be enough. Also, let’s present it in terms of percentages. Multiplying this figure by a 100 and rounding it to decimal points would us what we want: percentage points.\n\n# democracy rate:\ndem_rate &lt;- table(df$regime_type)[2] / sum(table(df$regime_type))\ndem_perc &lt;- dem_rate * 100\ndem_perc &lt;- round(dem_perc, 2)\n\n# in percentages:\ndem_perc\n\nDemocracy \n    42.77 \n\n\nWe can do the same for autocracies.\n\n# autocracy rate:\naut_rate &lt;- table(df$regime_type)[1] / sum(table(df$regime_type))\naut_perc &lt;- aut_rate * 100\naut_perc &lt;- round(aut_perc, 2)\n\n# in percentages:\naut_perc\n\nAutocracy \n    57.23 \n\n\nLet’s put them together in an object\n\nregime_perc &lt;- c(aut_perc, dem_perc)\nregime_perc \n\nAutocracy Democracy \n    57.23     42.77 \n\n\nLet’s do a quality check: these two numbers should add up roughly to 100.\n\nsum(regime_perc)\n\n[1] 100\n\n\nNow, we can do a barplot using percentages.\n\nbarplot(regime_perc)\n\n\n\n\n\n\n\n\nLet’s try to make it prettier.\n\nbarplot(regime_perc,\n        ylim = c(0, 60), # y-axis limits (coverage is from 0 to 60 )\n        ylab = \"Percentage of Countries\",\n        main = \"Distribution of Regime Type in 2010\"\n        )\n\n\n\n\n\n\n\nFigure 6.2: Percentage of autocracies and democracies across the world\n\n\n\n\n\nFigure 6.2 looks nice. We can present such a nice graph in our reports.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html#explanatory-variable-natural-resource-production",
    "href": "06_crosstab.html#explanatory-variable-natural-resource-production",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "6.3 Explanatory variable: natural resource production",
    "text": "6.3 Explanatory variable: natural resource production\nNext, let’s move to our explanatory variable: natural resource production (Natural_resources_rents_perc_of_GDP). This variable gives us the share of natural resources in a country’s economic output. Again, I will start with a quick summary.\n\nsummary(df$Natural_resources_rents_perc_of_GDP)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n  0.000   1.015   3.815   9.043  12.000  53.660       4 \n\n\nThis variable measures the share in terms of percentage, which means it should be between 0 and 100. As we expected, all the numbers are between 0 and 100. the maximum is 52.660 and the minimum is 0. There are no weird figures such as below 0 or above 100. However, we should be careful. There are 4 missing values.\nLet’s check the countries with the missing data.\n\n# Give me the country name such that natural resource variable is not available\ndf$Country_Name[ is.na(df$Natural_resources_rents_perc_of_GDP)]\n\n[1] \"Korea, Dem\" \"Somalia\"    \"Syrian Ara\" \"Timor-Lest\"\n\n\nWhat about the top 10 countries that are most dependent on natural resources?\n\n# find the countries that are most dependent on natural resources (top 10)\ndf |&gt; \n  select(Country_Name, Natural_resources_rents_perc_of_GDP) |&gt; # selects only two variables\n  arrange(desc(Natural_resources_rents_perc_of_GDP)) |&gt; # arranges (sorts) from highest to lowest\n  head(n = 10) # select only the first 10 rows\n\n   Country_Name Natural_resources_rents_perc_of_GDP\n1         Libya                               53.66\n2    Congo, Rep                               49.14\n3    Mauritania                               48.70\n4        Kuwait                               48.30\n5          Iraq                               41.48\n6    Saudi Arab                               40.93\n7        Angola                               39.04\n8      Mongolia                               38.66\n9          Oman                               37.14\n10   Equatorial                               34.71\n\n\nI am not going to check the lowest because there are many countries that do not produce any natural resources. This part is not interesting.\nLet’s visually summarize the variable. Starting with a histogram.\n\nhist(df$Natural_resources_rents_perc_of_GDP,\n     main = \"Natural resources as the % of GDP\", \n     xlab = \"Percentage of GDP\"\n     )\n\n\n\n\n\n\n\n\nWe can also produce a box plot.\n\nboxplot(df$Natural_resources_rents_perc_of_GDP,\n     main = \"Natural resources as the % of GDP\", \n     ylab = \"Percentage of GDP\"\n     )\n\n\n\n\n\n\n\n\nAgain, Natural_resources_rents_perc_of_GDP is a numerical variable, but we can create a categorical variable using it. This will allow us to do a cross tabulation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html#whether-a-country-is-a-major-natural-resource-producer-or-not",
    "href": "06_crosstab.html#whether-a-country-is-a-major-natural-resource-producer-or-not",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "6.4 Whether a country is a major natural resource producer or not",
    "text": "6.4 Whether a country is a major natural resource producer or not\nWe need to decide a cut-point to categorize countries according to whether they are a major natural resource producer or not. I suggest 5% because it is a nice round number. Looking at the data, it is also higher than the median.\nWe have two categories:\n\nA country is considered a major natural resource producer if natural resource production accounts for 5% or more of its GDP.\nA country is not considered a major natural resource producer if natural resource production accounts for less than 5% of its GDP.\n\n\n# There is no variable categorizing a country as a major natural resource producer or not\n# We need to create this\n# let's decide a threshold: 5 percent or over of the GDP\"\ndf$nat_res &lt;- NA\ndf$nat_res[df$Natural_resources_rents_perc_of_GDP &gt;= 5] &lt;- \"Major Producer\"\ndf$nat_res[df$Natural_resources_rents_perc_of_GDP &lt; 5]  &lt;- \"Not Major Producer\"\n\nLet’s see a frequency table.\n\n# table of the new variable (also show missing)\ntable(df$nat_res, useNA = \"always\")\n\n\n    Major Producer Not Major Producer               &lt;NA&gt; \n                69                 93                  4 \n\n\nLet’s do a quality control check if you wish.\n\n# check if categorization is correct:\n# are countries below %5 coded not major producer?\n# are countries above %5 coded major producer?\ntable(df$Natural_resources_rents_perc_of_GDP, df$nat_res)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html#cross-tabulate-regime_type-and-nat_res",
    "href": "06_crosstab.html#cross-tabulate-regime_type-and-nat_res",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "6.5 Cross tabulate regime_type and nat_res",
    "text": "6.5 Cross tabulate regime_type and nat_res\nWe reached to the stage that we can cross-tabulate to assess the relationship between natural resources and regime type. We will get closer to producing Table 6.1.\n\n\n\n\n\n\nImportant: The order is important\n\n\n\nThe order is important for reporting which percentages (row or column) are displayed. Traditionally, horizontal axis (x-axis) is reserved for the independent variable whereas the vertical axis (y-axis) is for the dependent variable. Horizontal property refers to rows in a cross tabulation. The dependent variable should go to columns.\n\n\n\n# cross tabulate:\n# first variable is for rows: explanatory variable\n# second variable is for column: outcome variable\ntable(df$nat_res, df$regime_type)\n\n                    \n                     Autocracy Democracy\n  Major Producer            55        14\n  Not Major Producer        37        56\n\n\nWe have produced the raw frequencies! Next, we need to calculate respective percentages.\nWe have two questions:\n\nWhat is the proportion of democracies among the countries that are major natural resource producers?\nWhat is the proportion of democracies among the countries that are not major natural resource producers?\n\nIf the proportion of democracy is much lower among the countries that are major natural resource producers than the countries that are not major resource producers, then there is a relationship in the direction of what the resource curse theory expects.\nThere are \\(55 + 14 = 69\\) major resource producers. Only \\(14 / 69 = 0.20\\) of them are democracies whereas \\(0.80\\) of them are autocracies.\nLet’s ask R to calculate it for us.\n\n# Major resource producers are the first row\ntable(df$nat_res, df$regime_type)[1, ]\n\nAutocracy Democracy \n       55        14 \n\n# sum of the first row is the number of natural resource producers\nsum(table(df$nat_res, df$regime_type)[1, ])\n\n[1] 69\n\n# respective proportions of regime type for natural resource producers\ntable(df$nat_res, df$regime_type)[1, ] / sum(table(df$nat_res, df$regime_type)[1, ])\n\nAutocracy Democracy \n0.7971014 0.2028986 \n\n# Let's keep this in an object:\nnatres_yes &lt;- table(df$nat_res, df$regime_type)[1, ] / sum(table(df$nat_res, df$regime_type)[1, ])\n\n# multiply it with 100 to make it percentages\nnatres_yes &lt;- natres_yes * 100\n\n# round it to two decimal points\nnatres_yes &lt;- round(natres_yes, 2)\n\n# see the output\nnatres_yes\n\nAutocracy Democracy \n    79.71     20.29 \n\n# sum of the two should be ~100\nsum(natres_yes)\n\n[1] 100\n\n\nNext, we will do the same for countries that are not major natural resource producers.\n\n# Not major resource producers are the second row\ntable(df$nat_res, df$regime_type)[2, ]\n\nAutocracy Democracy \n       37        56 \n\n# sum of the second row is the number of countries that are not major resource producers\nsum(table(df$nat_res, df$regime_type)[2, ])\n\n[1] 93\n\n# respective proportions of regime type for not natural resource producers\ntable(df$nat_res, df$regime_type)[2, ] / sum(table(df$nat_res, df$regime_type)[2, ])\n\nAutocracy Democracy \n0.3978495 0.6021505 \n\n# Let's keep this in an object:\nnatres_no &lt;- table(df$nat_res, df$regime_type)[2, ] / sum(table(df$nat_res, df$regime_type)[2, ])\n\n# multiply it with 100 to make it percentages\nnatres_no &lt;- natres_no * 100\n\n# round it to two decimal points\nnatres_no &lt;- round(natres_no, 2)\n\n# see the output\nnatres_no\n\nAutocracy Democracy \n    39.78     60.22 \n\n# sum of the two should be ~100\nsum(natres_no)\n\n[1] 100\n\n\n\n# Put the two together:\ntbl_percentages &lt;- rbind(natres_yes, natres_no)\ntbl_percentages\n\n           Autocracy Democracy\nnatres_yes     79.71     20.29\nnatres_no      39.78     60.22\n\n\nNow we have everything to produce the table we want to produce.\n\n# frequencies:\ntable(df$nat_res, df$regime_type)\n\n                    \n                     Autocracy Democracy\n  Major Producer            55        14\n  Not Major Producer        37        56\n\n# percentages\nrbind(natres_yes, natres_no)\n\n           Autocracy Democracy\nnatres_yes     79.71     20.29\nnatres_no      39.78     60.22\n\n\n\n\n\nTable 6.2: Democracy and Natural Resources\n\n\n\n\n\n\n\n\n\n\n\n\nAutocracy\nDemocracy\nTotal\n\n\n\n\nMajor Resource Producer\n55 (79.71%)\n14 (20.29%)\n69 (100%)\n\n\nNot Major Producer\n37 (39.78%)\n56 (60.22%)\n93 (100%)\n\n\nTotal\n92 (56.79%)\n70 (43.21%)\n162 (100%)\n\n\n\n\nRow percentages in parentheses sum up to 100%\n\n\n\n\nThe rate of democracy is much lower (20.29%) among major natural resource producers compared to countries that are not major resource producers (60.22% of them are democracies).\n\n6.5.1 A shortcut for calculating percentages\nWe calculated the respective percentages through bit by bit. This is a good way of learning. There are also shortcuts to carry out the same calculations. For example, prop.table() function is useful to calculate proportions in a table.\nLet’s start playing with prop.table().\n\n# table for natural resources and regime type\ntbl_nr &lt;- table(df$nat_res, df$regime_type)\n\n# see the table\ntbl_nr\n\n                    \n                     Autocracy Democracy\n  Major Producer            55        14\n  Not Major Producer        37        56\n\n# prop.table() gives the cell proportions (when the margin is left undefined)\nprop.table(tbl_nr)\n\n                    \n                      Autocracy  Democracy\n  Major Producer     0.33950617 0.08641975\n  Not Major Producer 0.22839506 0.34567901\n\n# summation of these proportions \n\nWe have produced cell proportions, but this is not we want to report. We want to calculate row proportions (turn these into percentages and report them). We will use the margin option.\n\n# prop.table() gives the cell proportions (when the margin is left undefined)\nprop.table(tbl_nr, margin = 1) # margin 1 refers to rows \n\n                    \n                     Autocracy Democracy\n  Major Producer     0.7971014 0.2028986\n  Not Major Producer 0.3978495 0.6021505\n\n\n\n# Percentages\nprop.table(tbl_nr, margin = 1) * 100 \n\n                    \n                     Autocracy Democracy\n  Major Producer      79.71014  20.28986\n  Not Major Producer  39.78495  60.21505\n\n# round them\nround(prop.table(tbl_nr, margin = 1) * 100, 2)\n\n                    \n                     Autocracy Democracy\n  Major Producer         79.71     20.29\n  Not Major Producer     39.78     60.22\n\n# keep them in an object\ntbl_nr_prc &lt;- round(prop.table(tbl_nr, margin = 1) * 100, 2)\n\n\n\n6.5.2 Visualization\nCreating a stacked bar graph is a good way of visualizing the correlation between regime type and natural resources.\n\n\n\n\n\n\n\n\nFigure 6.3: The number of countries by regime type\n\n\n\n\n\nLet’s break this apart one by one. First, the category on the x-axis is whether a country is a major natural resource producer or not. Second, the category shown by the stacked bars (color shading) refers to regime type. The two top bar chunks (in sky blue) represents democracy and the two lower bar chunks (in salmon red) are for autocracy.\nLet’s try to generate this. What if we plug in a barplot() to our table?\n\n# barplot function directly takes the table tbl_nr_prc as input:\nbarplot(tbl_nr_prc)\n\n\n\n\n\n\n\n\nOk, this is a start, but we are a little bit off. Most notably, x-axis categories refer to regime type. This needs to be swapped. Also, bars go higher than 100%, which is weird. This is because barplot() is stacking column wise, but our input is row wise. To swap it, we can simply use a transpose function: t(). This will swap the rows and columns.\n\n# swapping rows and columns: let's see if it is working\nt(tbl_nr_prc)\n\n           \n            Major Producer Not Major Producer\n  Autocracy          79.71              39.78\n  Democracy          20.29              60.22\n\n# yes it is working!\n# feed this into barplot\nbarplot(t(tbl_nr_prc))\n\n\n\n\n\n\n\n\nWe have created the substantive part. Next, add the legend.\n\nbarplot(t(tbl_nr_prc),\n        legend.text = colnames(tbl_nr_prc)\n        )\n\n\n\n\n\n\n\n\nFinally, we can add titles.\n\nbarplot(t(tbl_nr_prc),\n        main = \"Regime Type and Natural Resources\",\n        ylab = \"Percentage of Countries\",\n        legend.text = colnames(tbl_nr_prc)\n        )\n\n\n\n\n\n\n\n\nYou could also change the colors if you like. We have already covered how to do this.\nWhen reporting, do not forget to put figure captions and numbers (see Figure 6.3).\nHow to write it out?\n\n# writing this graph into a .png file\npng(\"stacked.png\", res = 300, units = \"cm\", width = 12, height = 16)\nbarplot(t(tbl_nr_prc),\n        main = \"Regime Type and Natural Resources\",\n        ylab = \"Percentage of Countries\",\n        legend.text = colnames(tbl_nr_prc)\n        )\ndev.off()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "06_crosstab.html#task-natural-resources-and-civil-conflict",
    "href": "06_crosstab.html#task-natural-resources-and-civil-conflict",
    "title": "6  Resource Curse Theory: Natural Resources and Democracy",
    "section": "6.6 Task: Natural Resources and Civil Conflict",
    "text": "6.6 Task: Natural Resources and Civil Conflict\nThis is a challenge for you: assess the relationship between natural resources and civil conflict. The resource curse theory also posits that countries that rely heavily on natural resource production are vulnerable to internal armed conflict. \\[ Natural \\; resources \\rightarrow Civil \\; Conflict \\]\nTake a look at the CivilConflict variable. It has three categories: civil war; minor civil conflict; no conflict. Using CivilConflict and nat_res, answer the questions below.\n\nIs there a relationship between natural resources and civil conflict?\n\nCross tabulate CivilConflict and nat_res.\nFor major natural resource producers, report the percentage of countries that are peaceful, in civil war, or facing minor conflict.\nFor countries that are not major natural resource producers, report the percentages for respective conflict categories.\nComparing these percentages, make a decision: is there a relationship between natural resources and civil conflict?\n\nUsing CivilConflict variable, create a new binary variable called conflict_bin, which indicates whether a country is peaceful or in conflict. Repeat the question #1 for this conflict_bin variable.\nCreate a stacked bar graph similar to Figure 6.4.\n\n\n\n\n\n\n\n\n\nFigure 6.4: Percentage of countries in conflict by natural resource production",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Cross Tabulation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html",
    "href": "07_correlation.html",
    "title": "7  Correlation between Numerical Variables",
    "section": "",
    "text": "7.1 Infant mortality rate and Female life expectancy\nWe start with two numerical variables that we expect to be highly correlated: infant mortality rate and female life expectancy. These two are both related to health outcomes: developed countries with good healthcare have low infant mortality and high female life expectancy whereas underdeveloped countries with limited or no healthcare have high infant mortality and low female life expectancy.\nWe start with this example because we know they are highly correlated (recall the lecture slides).\nAlthough we expect the two variables to be highly correlated, we don’t think there is a causal relationship. There is an underlying factor, the health care quality, which influences both variables of interest as seen in Figure 7.1.\nIt is always a good idea to explore the variables of interest one by one. Recall that we called this univariate analysis. It refers to doing a numerical summary (descriptive statistics) and some visual exploration.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#infant-mortality-rate-and-female-life-expectancy",
    "href": "07_correlation.html#infant-mortality-rate-and-female-life-expectancy",
    "title": "7  Correlation between Numerical Variables",
    "section": "",
    "text": "flowchart LR\n  Z[Health care] --&gt; X[Female life expectancy]\n  Z --&gt; Y[Infant mortality rate]\n\n\n\n\nFigure 7.1: Z influencing both X and Y: X and Y are correlated",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#univariate-analysis-female-life-expectancy",
    "href": "07_correlation.html#univariate-analysis-female-life-expectancy",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.2 Univariate analysis: Female life expectancy",
    "text": "7.2 Univariate analysis: Female life expectancy\n\n# Numerical summary of Female life expectancy\nsummary(df$Life_exp_female)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  48.88   64.41   74.91   71.94   78.73   86.30 \n\n\nThe highest value for Life_exp_female is 86.30 and the lowest value is 48.88. These are reasonable values for the life expectancy of humans. There is also a striking inequality between countries: almost 40 years of difference in life expectancy between the best and worst performing countries. This is a huge gap.\nFor completeness, I will also report the standard deviation.\n\n# calculate the standard deviation\nsd(df$Life_exp_female) |&gt; round(2)\n\n[1] 9.35\n\n\nLet’s continue with visual summaries.\n\n# histogram\nhist(df$Life_exp_female,\n     breaks = seq(45, 90, 2.5), #each bin covers 2.5 years (spanning from 45 to 90) \n     main = \"Distribution of Female Life Expectancy in 2010\",\n     xlab = \"Age\",\n     ylab = \"Number of Countries\",\n     xlim = c(45, 90) # x-axis span from 45 to 90\n     )\n\n\n\n\n\n\n\n\nWhat about a boxplot?\n\n# boxplot\nboxplot(df$Life_exp_female,\n        ylim = c(45, 90), # y-axis span from 45 to 90\n        ylab = \"Age\",\n        main = \"Female Life Expectancy in 2010\"\n        )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#univariate-analysis-infant-mortality-rate",
    "href": "07_correlation.html#univariate-analysis-infant-mortality-rate",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.3 Univariate analysis: Infant mortality rate",
    "text": "7.3 Univariate analysis: Infant mortality rate\nInfant_Mortality_Rate measures the number of infants per 1,000 live births who die before reaching one year of age.\n\n# Numerical summary\nsummary(df$Infant_Mortality_Rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2.00    7.50   19.20   29.25   48.10  108.40       1 \n\n\nAgain, let’s check if these values make sense. The values span from 2 (minimum) to 108.40 (maximum). The highest value is extremely high: more than 10% of newborns die within the first year. Although this is shockingly high, it is a possible value. In contrast, any negative value would tell us that there is something wrong with the data because the mortality rate cannot be below zero.\nWhich countries have the highest and lowest rates? We learned how to check these.\n\n# Countries with highest infant mortality\ndf |&gt; \n  select(Country_Name, Infant_Mortality_Rate) |&gt;\n  arrange(desc(Infant_Mortality_Rate)) |&gt;\n  head(5)\n\n  Country_Name Infant_Mortality_Rate\n1   Sierra Leo                 108.4\n2   Central Af                 101.9\n3      Somalia                  96.9\n4        Haiti                  87.0\n5         Chad                  85.2\n\n\n\n# Countries with lowest infant mortality\ndf |&gt; \n  select(Country_Name, Infant_Mortality_Rate) |&gt;\n  arrange(Infant_Mortality_Rate) |&gt;\n  head(5)\n\n  Country_Name Infant_Mortality_Rate\n1      Iceland                   2.0\n2    Singapore                   2.2\n3        Japan                   2.4\n4      Finland                   2.5\n5       Sweden                   2.5\n\n\nBe careful that one observation is missing. Which country is that?\n\n# missing data\ndf$Country_Name[is.na(df$Infant_Mortality_Rate)]\n\n[1] \"Kosovo\"\n\n\n\n# standard deviation: make sure to remove the missing value \nsd(df$Infant_Mortality_Rate, na.rm = TRUE)\n\n[1] 25.89212\n\n# Beware: if you fail to tell R to remove the missing value, you will get a missing value \nsd(df$Infant_Mortality_Rate)\n\n[1] NA\n\n\nLet’s continue with visual summaries.\n\n# Histogram:\nhist(df$Infant_Mortality_Rate, \n     main = \"Distribution of Infant Mortality Rate in 2010\",\n     ylab = \"Number of Countries\",\n     xlab = \"Mortality Rate (in 1000)\"\n     )\n\n\n\n\n\n\n\n\nLet’s see a box plot too:\n\nboxplot(df$Infant_Mortality_Rate, \n     main = \"Distribution of Infant Mortality Rate in 2010\",\n     ylab = \"Mortality Rate (in 1000)\"\n     )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#bivariate-analysis",
    "href": "07_correlation.html#bivariate-analysis",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.4 Bivariate analysis",
    "text": "7.4 Bivariate analysis\nNext, we explore Life_exp_female and Infant_Mortality_Rate together. Let’s start with a scatter plot!\nThere are two dimensions in a scatter plot: vertical (y-axis) and horizontal (x-axis). Traditionally, the outcome variable should go to the y-axis whereas the independent variable should go to the x-axis.\nIn this example, however, we are just exploring the relationship between two variables without designating any one of them as outcome or explanatory. It would not matter which variable goes to which axis.\n\n# Scatter plot of Infant Mortality Rate and Female Life Expectancy\nplot(df$Infant_Mortality_Rate, df$Life_exp_female) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that R showed the first variable (Infant_Mortality_Rate) on the x-axis whereas the second variable (Infant_Mortality_Rate) on the y-axis. This is all in line with the conventions we learned.\n\n\nJust by looking at this scatter plot, we can visually comprehend that there is a strong linear association between the variables! There is a negative relationship: the higher the infant mortality rate, the lower the female life expectancy. This is something we would strongly expect.\nThis scatter plot was a good start, but it could look more professional. Let’s add titles, etc.\n\nplot(df$Infant_Mortality_Rate, df$Life_exp_female,\n     main = \"World in 2010: Infant Mortality Rate and Female Life Expectancy\",\n     xlab = \"Infant Mortality Rate (in 1000)\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16, # change points from hollow circles to filled circle\n     cex = 0.5 # make the points smaller (half in size)\n     )\n\n\n\n\n\n\n\n\nFinally, let’s calculate the correlation coefficient. Beware that one observation is missing! Because of this, we must make sure that R only considers pairwise complete observations.\n\n# Correlation coefficient:\ncor(df$Infant_Mortality_Rate, df$Life_exp_female, use = \"pairwise.complete\")\n\n[1] -0.9420862\n\n\nThe correlation coefficient is \\(-0.94\\). This value is close to the lowest extreme, \\(-1\\). There is a very strong linear correlation between the two variables!\nLet’s also see a very strong positive correlation. Male and female life expectancy should be highly correlated. This is common sense expectation: in countries where women are expected to live long, men are also expected to live long.\nLet’s see if that is the case.\n\n# scatter plot of male and female life expectancy\nplot(df$Life_exp_male, df$Life_exp_female,\n     main = \"World in 2010: Life Expectancy by Gender\",\n     xlab = \"Male Life Expectancy\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16, # change points from hollow circles to filled circle\n     cex = 0.5 # make the points smaller (half in size)\n     )\n\n\n\n\n\n\n\n# Correlation coefficient:\ncor(df$Life_exp_male, df$Life_exp_female, use = \"pairwise.complete\") |&gt; round(2)\n\n[1] 0.97\n\n\nThis is a very strong positive correlation as we expected.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#example-no-linear-association",
    "href": "07_correlation.html#example-no-linear-association",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.5 Example: No linear association",
    "text": "7.5 Example: No linear association\nNext, let’s see an example where there is very weak linear correlation.\nIs there a linear relationship between female life expectancy and voter turnout in the last election? I don’t think there is a reason for two variables to be correlated.\n\n# Turnout variable summary\nsummary(df$Turnout)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.1800  0.5325  0.6700  0.6576  0.7700  1.0000       8 \n\n# Correlation between Turnout and Female Life Expectancy\ncor(df$Turnout, df$Life_exp_female, use = \"pairwise.complete\") |&gt; round(2)\n\n[1] 0.06\n\n\nThe correlation coefficient is 0.06. This is pretty close to 0. Compared to Infant_Mortality_Rate or Life_exp_male, there is a very weak association.\nLet’s visually display.\n\n# a scatter plot:\nplot(df$Life_exp_female, df$Turnout,\n     main = \"Female Life Expectancy and Voter Turnout (2010)\",\n     xlab = \"Female Life Expectancy (Years)\",\n     ylab = \"Voter Turnout (%)\",\n     pch = 16\n     )",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#task-1-resource-curse-theory-revisited",
    "href": "07_correlation.html#task-1-resource-curse-theory-revisited",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.6 Task #1: Resource curse theory revisited",
    "text": "7.6 Task #1: Resource curse theory revisited\nWe are now returning back to the resource curse theory, which is something we worked on last week. Recall that this theory expects natural resources to hinder democracy. Is there a relationship between higher rates of natural resources (as % of GDP) and lower levels of democracy?\nAs we have seen last week, the dataset comes with two numerical variables, v2x_polyarchy and Natural_resources_rents_perc_of_GDP. There is no need to do any data transformation, as we need them in their numerical form.\nYour task is to explore the relationship between the two. More specifically, you should:\n\nPresent a scatter plot of v2x_polyarchy and Natural_resources_rents_perc_of_GDP. It should look like in Figure 7.2\nReport the correlation coefficient.\n\n\n\n\n\n\n\n\n\nFigure 7.2: the World in 2010 Dataset. V-Dem Polyarchy Index is between 0 (the least democratic) and 1 (the most democratic).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "07_correlation.html#task-2-explore-the-modernization-theory",
    "href": "07_correlation.html#task-2-explore-the-modernization-theory",
    "title": "7  Correlation between Numerical Variables",
    "section": "7.7 Task #2: Explore the Modernization Theory",
    "text": "7.7 Task #2: Explore the Modernization Theory\nFinally, we turn to modernization theory, which posits that economic development leads to democratization. This theory expects to find a correlation between economic development and democracy.\n\nExplore the variable GDP_pc_PPP. This variable is GDP per capita (at purchasing power parity). Use this as a proxy measurement for economic development.\n\nPresent appropriate numerical and visual summaries. For numerical summary, you should report n, mean, sd, and the five-point summary. See Table 7.1 for how it should look like.\n\nPresent a scatter plot v2x_polyarchy and GDP_pc_PPP.\nReport the correlation coefficient.\n\n\n\n\n\n\nTable 7.1: Descriptive Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nN\nMin\nQ1\nMedian\nMean\nQ3\nMax\nSD\n\n\n\n\nGDP per capita (PPP)\n161\n646.86\n3127.02\n9627.94\n15494.4\n20497.93\n122609.4\n17154.74",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "08_regression.html",
    "href": "08_regression.html",
    "title": "8  Regression",
    "section": "",
    "text": "8.1 Preliminaries\n# Preliminaries ####\n# remove everything from the environment\nrm(list = ls())\n\n# set the working directory:\nsetwd(\"~/methods/\") # adjust the directory address accordingly\n# require the packages \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nNext, import the dataset World in 2010.\n# Import the dataset: World in 2010 ####\n# import the data:\ndf &lt;- read.csv(\"data/world_in_2010.csv\")",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#female-life-expectancy-and-mortality-rate",
    "href": "08_regression.html#female-life-expectancy-and-mortality-rate",
    "title": "8  Regression",
    "section": "8.2 Female Life Expectancy and Mortality Rate",
    "text": "8.2 Female Life Expectancy and Mortality Rate\nWe will explore the relationship between Infant Mortality Rate and Female Life Expectancy. Remember, there is no reason to assume that one variable causes the other. Since there is no clear outcome or explanatory variable, we will simply designate one as the predicted variable and the other as the predictor. As we did last week, let’s place Infant Mortality Rate on the x-axis and Female Life Expectancy on the y-axis.\n\nplot(df$Infant_Mortality_Rate, df$Life_exp_female,\n     main = \"World in 2010: Infant Mortality Rate and Female Life Expectancy\",\n     xlab = \"Infant Mortality Rate (in 1000)\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16, # change points from hollow circles to filled circle\n     cex = 0.5 # make the points smaller (half in size)\n     )\n\n\n\n\n\n\n\nFigure 8.1: The scatter plot we created last week",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#regression-equation",
    "href": "08_regression.html#regression-equation",
    "title": "8  Regression",
    "section": "8.3 Regression Equation",
    "text": "8.3 Regression Equation\nNext, we are going to fit a regression line to Figure 8.1. Let’s type our regression equation first:\n\\[ FemLifeExp = \\alpha + \\beta * InfMortRate  \\tag{8.1}\\]\nFor simplicity we could also write it as: \\[ Y = \\alpha + \\beta * X  \\tag{8.2}\\]\nwhere \\(Y\\) is Life_exp_female and \\(X\\) is Infant_Mortality_Rate.\nEquation 8.2 represents the generic equation we will use for all regression analyses (with some adjustments when required).\nWhen we are telling R to fit the best line, we are asking it to estimate the coefficients of \\(\\alpha\\) and \\(\\beta\\).\nFor running a regression, we will use the base R function lm(), which stands for linear model. Following the structure in {#eq-form1}, I need to specify my predicted variable, Life_exp_female. R knows that an \\(\\alpha\\) is needed, so we do not need to type anything for it. My predictor variable is Infant_Mortality_Rate. To separate the left and right hand-side of the equation, I use a tilda (~). You can read this formula as “Female Life Expectancy as a function of Infant Mortality Rate”.\n\n# linear model : #### \nlm(df$Life_exp_female ~ df$Infant_Mortality_Rate)\n\n\nCall:\nlm(formula = df$Life_exp_female ~ df$Infant_Mortality_Rate)\n\nCoefficients:\n             (Intercept)  df$Infant_Mortality_Rate  \n                 81.9260                   -0.3414  \n\n\nR brought us the coefficients for \\(\\alpha = 81.9260\\) and \\(\\beta = -0.3414\\). Recall that \\(\\alpha\\) is also called the intercept.\n\n\n\n\n\n\nMore common code: without the $ notation\n\n\n\nWe can also specify the dataset and just type the variables (without using the $ notation). We will most often type in this way.\n\n# linear model :\nlm(Life_exp_female ~ Infant_Mortality_Rate, data = df)\n\n\nCall:\nlm(formula = Life_exp_female ~ Infant_Mortality_Rate, data = df)\n\nCoefficients:\n          (Intercept)  Infant_Mortality_Rate  \n              81.9260                -0.3414  \n\n\n\n\nWe can store a regression output as an object. Let’s store our first regression as reg_mod1.\n\n# linear model :\nreg_mod1 &lt;- lm(Life_exp_female ~ Infant_Mortality_Rate, data = df)\n\n# see the object reg_mod1\nreg_mod1 \n\n\nCall:\nlm(formula = Life_exp_female ~ Infant_Mortality_Rate, data = df)\n\nCoefficients:\n          (Intercept)  Infant_Mortality_Rate  \n              81.9260                -0.3414  \n\n\n\n# direct access to coefficients as numerical values:\nreg_mod1$coefficient |&gt; round(2)\n\n          (Intercept) Infant_Mortality_Rate \n                81.93                 -0.34 \n\n\nWe discussed these coefficients in detail during the last lecture, but let’s remind ourselves what they mean. Let’s write our equation.\n\\[ FemLifeExp = 81.93 - 0.34 * InfMortRate \\]\nThe intercept is \\(81.93\\) which is the expected value of Life_exp_female when Infant_Mortality_Rate is 0.\n\\[\\begin{align*}\n\nFemLifeExp &= 81.93 - 0.34 * 0 \\\\\n&= 81.93\n\n\\end{align*}\\]\nNext, let’s discuss \\(\\beta = - 0.34\\). One unit of increase in Infant_Mortality_Rate is associated with \\(0.34\\) units of decrease in the expected value of Life_exp_female.\n\nFor a country with an Infant_Mortality_Rate of 1, the expected Life_exp_female:\n\n\\[\\begin{align*}\n\nFemLifeExp &= 81.93 - 0.34 * 1 \\\\\n&= 81.59\n\n\\end{align*}\\]\n\nFor a country with an Infant_Mortality_Rate of 2, the expected Life_exp_female:\n\n\\[\\begin{align*}\n\nFemLifeExp &= 81.93 - 0.34 * 2 \\\\\n&=  81.93 - 0.68 \\\\\n&=  81.25\n\n\\end{align*}\\]\n\nFor a country with an Infant_Mortality_Rate of 10, the expected Life_exp_female:\n\n\\[\\begin{align*}\n\nFemLifeExp &= 81.93 - 0.34 * 10 \\\\\n&=  81.93 - 3.4 \\\\\n&=  78.53\n\n\\end{align*}\\]\nWe can see the negative relationship: the higher the Infant_Mortality_Rate the lower the expected Life_exp_female.\nRegression is a powerful tool because it also gives us the estimated magnitude of the relationship. As we have seen in this example, we can\n\nOne unit of increase in Infant_Mortality_Rate is associated with \\(0.34\\) units of decrease in the expected value of Life_exp_female.\n\nCorrelation coefficient is a measure of the strength of linearity of the relationship between two variables. However, it does not provide information about the substantive magnitude of that relationship.\nRegression, on the other hand, estimates the substantive magnitude of a relationship between one outcome variable and multiple predictor variables (more on this next week). It can also be used to model non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#visualising-the-regression-line",
    "href": "08_regression.html#visualising-the-regression-line",
    "title": "8  Regression",
    "section": "8.4 Visualising the Regression Line",
    "text": "8.4 Visualising the Regression Line\nWe can visually display the estimated regression line on the scatter plot. We just need to use the abline() function. Let’s feed the reg_mod1 into the abline() function. I will also specify the thickness of the line by using the argument lwd. It will make the line three times thicker for better visibility.\n\n# Plotting the regression line ####\nplot(df$Infant_Mortality_Rate, df$Life_exp_female,\n     main = \"World in 2010: Infant Mortality Rate and Female Life Expectancy\",\n     xlab = \"Infant Mortality Rate (in 1000)\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16, # change points from hollow circles to filled circle\n     cex = 0.5 # make the points smaller (half in size)\n     ) \n# add the regression line on to the plot: \nabline(reg_mod1, # reg_mod1 is the main input: intercept and slope. \n       lwd = 3   # lwd is for making the line thicker\n       )  \n\n\n\n\n\n\n\nFigure 8.2: Regression line",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#predict-function",
    "href": "08_regression.html#predict-function",
    "title": "8  Regression",
    "section": "8.5 predict() Function",
    "text": "8.5 predict() Function\nAs we have seen, the regression equation allows us to calculate predicted values of \\(Y\\) (Life_exp_female, in this example) given \\(X\\) (Infant_Mortality_Rate). We could calculate predicted values one-by-one as we did for X values of 1, 2, and 10. However, asking R to do it for us is much faster and easier.\nFor this objective, we will use the predict() function, which returns the predicted (or expected) values of \\(Y\\) given the explanatory variable(s).\nIf you directly plug our model (reg_mod1) into the predict() function, the model will generate predicted values of the outcome variable for realizations of Infant_Mortality_Rate (i.e. actual data points). Let’s display it on our screen.\n\n# Predicted values of Y given X ####\npredict(reg_mod1)\n\nThese are predictions based on our regression model and actual measurements Infant_Mortality_Rate. We can also give hypothetical values of Infant_Mortality_Rate. Previously, we plugged in 1, 2, and 10. Let’s do the same by using predict().\n\n# predict by giving new (hypothetical) data points ####\n# this is our initial predictions for X values of 1, 2 and 10\n\n# create a new data frame in which we are going to store our hypothetical Infant_Mortality_Rate values:\ndf_pred_ini &lt;-  data.frame(Infant_Mortality_Rate = c(1, 2, 10))\n\n# see this data frame: ini is abb for initial\ndf_pred_ini\n\n  Infant_Mortality_Rate\n1                     1\n2                     2\n3                    10\n\n\n\n# df_pred_initial is just a new data frame \n# it holds hypothetical Infant_Mortality_Rate values\nView(df_pred_ini)\n\nNext, plug in this new data to the predict() function.\n\n# prediction using 'new' data:\npredict(reg_mod1, newdata = df_pred_ini)\n\n       1        2        3 \n81.58465 81.24327 78.51224 \n\n# store it in the new df\ndf_pred_ini$predicted_fem_life_exp &lt;- predict(reg_mod1, newdata = df_pred_ini)\n\n\n# new data frame now holds:\n# predicted infant mortality rates given hypothetical Infant_Mortality_Rate\nView(df_pred_ini)\n\nIn this example, we just used a three hypothetical values for Infant_Mortality_Rate, namely 1,2, and 10. These are the values we used in our ‘manuel calculations’ using the regression equation and \\(\\alpha\\) and \\(\\beta\\) coefficients. We can plug in large number of hypothetical values if we like.\n\n# A series of hypothetical values\nx &lt;- seq(from = 0, to = 110, by = 5) \n\n# put it into a data frame\ndf_pred_big &lt;- data.frame(Infant_Mortality_Rate = x)\n\n# predictions\ndf_pred_big$pred_fem_life_exp &lt;- predict(reg_mod1, newdata = df_pred_big)\n\nSee your predicted values.\n\n# See the df_pred_big\nView(df_pred_big)\n\nNote that these predicted values are always on the regression line! If you plot them, they will form a line (i.e., the regression line).\n\nplot(x = df_pred_big$Infant_Mortality_Rate,\n     y = df_pred_big$pred_fem_life_exp,\n     main = \"Regression Analysis: Predicted Values\",\n     xlab = \"Infant Mortality Rate (in 1000)\",\n     ylab = \"Female Life Expectancy\"\n     )",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#regression-prediction-error",
    "href": "08_regression.html#regression-prediction-error",
    "title": "8  Regression",
    "section": "8.6 Regression Prediction Error",
    "text": "8.6 Regression Prediction Error\nNext, we will calculate the errors (also known as residuals), which are the differences between the actual data points (i.e., realizations) of \\(Y\\) and the predicted values of \\(Y\\).\n\\[\nError = Actual \\; Y - Predicted \\; Y\n\\]\nFor simplification of notation, we use \\(e\\) for error, \\(Y\\) for actual data points and Y-hat (\\(\\hat{Y}\\)) for predictions.\n\\[\ne = y - \\hat{y}\n\\]\nFor example, let’s look at Japan. What is Japan’s Infant_Mortality_Rate and Fem_Life_Exp? We can quickly ask R to bring it for us.\n\n# Infant Mortality Rate for Japan :\n# bring me Mortality rate such that Country Name is Japan:\ndf$Infant_Mortality_Rate[df$Country_Name == \"Japan\"] \n\n[1] 2.4\n\n# Female Life Expectancy for Japan\n# bring me Female Life Expectancy such that Country Name is Japan: \ndf$Life_exp_female[df$Country_Name == \"Japan\"]\n\n[1] 86.3\n\n\nJapan’s actual Female Life Expectancy is \\(86.3\\). We are going to compare this to the predicted value. Let’s calculate the predicted value for Japan:\n\\[\\begin{align*}\n\n\\hat{y} &= 81.93 - 0.34 * 2.4 \\\\\n&= 81.11\n\n\\end{align*}\\]\nI am very bad with calculations, so let’s ask R if we did it correctly.\n\n# x value for Japan: \nx_jpn   &lt;- df$Infant_Mortality_Rate[df$Country_Name == \"Japan\"] \n\n# make it a data frame to feed into predict() function:\ndf_jpn  &lt;- data.frame(Infant_Mortality_Rate = x_jpn)\n\n# use the predict function:\npredict(reg_mod1, newdata = df_jpn) |&gt; round(2)\n\n    1 \n81.11 \n\n\nNow we can calculate the error! It is the actual minus the predicted:\n\\[\\begin{align*}\n\ne &= y - \\hat{y} \\\\\n&= 86.3 - 81.11 \\\\\n&= 5.19\n\\end{align*}\\]\nThis difference is illustrated in Figure 8.3, where Japan’s data point is highlighted with a red circle. Notice that it is positioned above the corresponding regression line. Specifically, Japan’s actual \\(Y\\) value is \\(5.19\\) years higher than its predicted value.\n\n\nShow/Hide R Code for Plot for Japan\n# The code for generating Figure 8.3\n\n# Usual scatter plot: \nplot(df$Infant_Mortality_Rate, df$Life_exp_female,\n     main = \"Japan in the World in 2010 Data\",\n     xlab = \"Infant Mortality Rate (in 1000)\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16, \n     cex = 0.5 \n     )\n\n# add the regression line on to the plot: \nabline(reg_mod1, # reg_mod1 is the main input: intercept and slope. \n       lwd = 3   # lwd is for making the line thicker\n       )\n\n# now highlight the data point for Japan:\npoints(x = df$Infant_Mortality_Rate[df$Country_Name == \"Japan\"], # x value for Japan\n       y = df$Life_exp_female[df$Country_Name == \"Japan\"], # y value for Japan    \n       pch = 1, # default pch will circle the point\n       lwd  = 2,  # make the circle thicker \n       cex = 2,  # make the circle bigger\n       col = \"red\" # make the circle red! \n       )\n\n\n\n\n\n\n\n\nFigure 8.3: Japan is highlighted with a red circle\n\n\n\n\n\n\n8.6.1 Residual (error) for all countries\nWe used Japan as an illustrative example. Next, we will calculate the residuals (or errors) for all countries. We need to be careful because of handling any missing data. If you try to store the predicted values in your original data frame, your code will not run and you will get an error message.\n\ndf$yhat &lt;- predict(reg_mod1)\n\nError in `$&lt;-.data.frame`(`*tmp*`, yhat, value = c(`1` = 59.5315545866218, : replacement has 165 rows, data has 166\n\n\nThe error message says the replacement has 165 rows, data has 166. Let’s unpack this. Our original data frame has 166 observations (rows).\n\n# number of observations:\nnrow(df)\n\n[1] 166\n\n\nHowever, the model returns 165 predicted values.\n\n# the number of predicted values \npredict(reg_mod1) |&gt; length()\n\n[1] 165\n\n\nWhy is there a difference? Recall that one observation was missing for Infant_Mortality_Rate.\n\n# summary of variables of interest\nsummary(df$Infant_Mortality_Rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2.00    7.50   19.20   29.25   48.10  108.40       1 \n\n\nWhen R is running the requested regression, it automatically handles missingness by skipping the rows where at least one measurement is missing. In this case, the row for Kosovo is skipped because its Infant_Mortality_Rate is not available.\nWhile working with residuals, it is practical to create a smaller data frame with only the variables that are used in the regression. We can also remove the any missing observations.\n\n# Using tidyverse select only the variables of interest (and country name to identify observations):\ndf_reg &lt;- df |&gt; \n  select(Country_Name, Life_exp_female, Infant_Mortality_Rate) |&gt;\n  na.omit() # remove rows that contain any missingness\n\n# check the number of rows: \n# it should be one lower than the original data\n# because one row (for Kosovo) is removed\nnrow(df_reg)\n\n[1] 165\n\n\nLet’s try again to generate predicted values.\n\n# predicted values\ndf_reg$predicted &lt;- predict(reg_mod1, newdata = df_reg)\n\n\n# see your new data frame with predicted values:\nView(df_reg)\n\nIt is time to calculate the residuals for all countries.\n\n# calculate the residuals (errors):\ndf_reg$error &lt;- df_reg$Life_exp_female - df_reg$predicted\n\nYou can explore your model’s predictions by examining the errors. For instance, which countries does the model predict inaccurately? In other words, which countries have a Life_exp_fem that is higher (or lower) than expected, given their Infant_Mortality_Rate? We can explore these.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#task-1-lecture-example-marks-and-study-time",
    "href": "08_regression.html#task-1-lecture-example-marks-and-study-time",
    "title": "8  Regression",
    "section": "8.7 Task #1: Lecture Example (Marks and Study Time)",
    "text": "8.7 Task #1: Lecture Example (Marks and Study Time)\nUsing the data from the lecture slides, answer the questions below using R.\n\n# lecture example data:\nlect &lt;- data.frame(\n  mark = c(37, 30, 45, 48, 63, 57, 65, 55, 50, 70),\n  study_time = c(rep(1, 4), rep(4, 6))\n  )\n\n\nPerform a regression of mark on study_time. Report the alpha and beta.\nI plan to study 7 hours in the next exam. Using the regression model, calculate my expected mark?\nWhat is my expected mark if I do not study at all?\nWhat is the mean of mark?\n\nunconditional mean\nmean conditional on 1 hour of study_time\nmean conditional on 4 hours of study_time\n\nPlot mark and study_time. X-axis should span from 0 to 6 hours of study. Y-axis should be from 0 to 100. Draw the regression line.\nChallenge: On the plot you created for question 5, show that conditional means are on the regression line.\n\n\nAnswers\n\nPerform a regression of mark on study_time. Report the alpha and beta.\n\n\n\nShow the answer for Question 1\n# regression for task 1\nreg_t1 &lt;- lm(mark ~ study_time, data = lect)\nreg_t1\n# The intercept is 33.33\n# The beta is 6.67\n\n\n\nI plan to study 7 hours in the next exam. Using the regression model, calculate my expected mark?\n\n\n\nShow the answer for Question 2\npredict(reg_t1,  newdata = data.frame(study_time = 7))\n\n\n\nWhat is my expected mark if I do not study at all?\n\n\n\nShow the answer for Question 3\n# This is the intercept. It is 33.333 but let's display it on screen:\nreg_t1$coefficients[1]\n\n\n\nWhat is the mean of mark? Unconditional and conditional means.\n\n\n\nShow the answer for Question 4\n# unconditional mean:\nmean(lect$mark)\n\n# mean conditional on X = 1:\nmean(lect$mark[lect$study_time == 1])\n\n# mean conditional on X = 4:\nmean(lect$mark[lect$study_time == 4])\n\n\n\nPlot mark and study_time. Draw a regression line.\n\n\n\nShow the answer for Question 5\n# plot:\nplot(x = lect$study_time, \n     y = lect$mark,\n     xlim = c(0, 6),\n     ylim = c(0, 100),\n     xlab = \"Study Time\",\n     ylab = \"Mark\",\n     main = \"Data: Example from the Lecture\",\n     pch = 16)\n# add the regression line:\nabline(reg_t1, lwd = 2)\n\n\n\nChallenge: On the plot you created for question 5, show that the conditional means are on the regression line.\n\n\n\nShow the answer for Question 6\n# points for conditional means:\n# first point for x = 1:\np1 &lt;- data.frame(x = 1,\n                 y = mean(lect$mark[lect$study_time == 1])\n                 )\n\n# second point for x = 4:\np2 &lt;- data.frame(x = 4,\n                 y = mean(lect$mark[lect$study_time == 4])\n                 )\n                 \n### plotting part ###\n# the original plot:\nplot(x = lect$study_time, \n     y = lect$mark,\n     xlim = c(0, 6),\n     ylim = c(0, 100),\n     xlab = \"Study Time\",\n     ylab = \"Mark\",\n     main = \"Data: Example from the Lecture\",\n     pch = 16)\n# add the regression line:\nabline(reg_t1, lwd = 2)\n\n# add the points: \npoints(x = p1$x, y = p1$y, col = \"red\", pch = 15) # points for X = 1\npoints(x = p2$x, y = p2$y, col = \"red\", pch = 15) # points for X = 4",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#task-2-female-life-expectancy-and-male-life-expectancy",
    "href": "08_regression.html#task-2-female-life-expectancy-and-male-life-expectancy",
    "title": "8  Regression",
    "section": "8.8 Task #2: Female Life Expectancy and Male Life Expectancy",
    "text": "8.8 Task #2: Female Life Expectancy and Male Life Expectancy\nRun a regression of Life_exp_female on Life_exp_male and perform the following analyses:\n\nInterpret the regression coefficients.\nVisualise the regression line: Create a scatter plot of the data and overlay the regression line to illustrate the relationship.\nCalculate the predicted values of Life_exp_female. Use the regression model to predict Life_exp_female for all countries based on their Life_exp_male.\nCalculate the errors (residuals): Compute the difference between the actual and predicted values of Life_exp_female.\nIdentify countries with higher-than-expected Life_exp_female: which countries have Life_exp_female more than five years abo the predicted values, given their Life_exp_male?.\nIdentify countries with lower-than-expected Life_exp_female: which countries have Life_exp_female values more than two years below the predicted values, given their Life_exp_male?.\n\n\n\nShow the answer for Task 2\n# regression:\nreg_t2 &lt;- lm(Life_exp_female ~ Life_exp_male, data = df)\n\n# regression coefficients: \nreg_t2\n\n# One year (unit) increase in Life_exp_male is associated with 1.084 years of increase in Life_exp_female\n# No need to try interpreting the intercept substantively. \n# Intercept is not meaningful in this case. No substantive interpretation. \n# Technically, when the life expectancy of men is 0 years old, the female life expectancy is -0.771 (this is not really meaningful). \n\n\n# plot:\nplot(df$Life_exp_male, df$Life_exp_female,\n     xlab = \"Male Life Expectancy\",\n     ylab = \"Female Life Expectancy\",\n     pch = 16\n     )\n# add the regression line: \nabline(lm(reg_t2), lwd = 3)\n\n# Use the regression model to predict `Life_exp_female` for all countries based on their `Life_exp_male`:\ndf$predicted_fem_life &lt;- predict(reg_t2)\n\n# errors:\ndf$errors &lt;- df$Life_exp_female - df$predicted_fem_life \n\n# Which countries have `Life_exp_female` more than five years abo the predicted values, given their `Life_exp_male`?.\ndf$Country_Name[df$errors &gt; 5]\n\n# Which countries have `Life_exp_female` values more than two years below the predicted values, given their `Life_exp_male`?.\ndf$Country_Name[df$errors &lt; -2]",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "08_regression.html#task-3-challenge-imputation",
    "href": "08_regression.html#task-3-challenge-imputation",
    "title": "8  Regression",
    "section": "8.9 Task #3: Challenge: Imputation",
    "text": "8.9 Task #3: Challenge: Imputation\nRecall that Kosovo’s Infant_Mortality_Rate is missing in the World in 2010 dataset. Using a simple regression model with Life_exp_female as the only predictor, estimate the Kosovo’s Infant_Mortality_Rate.\n\n\n\n\n\n\nTip: Unhide to see the tip\n\n\n\n\n\nPredicted variable is the outcome variable.\nYour outcome variable should be Infant_Mortality_Rate and your independent variable is Life_exp_female.\n\n\n\n\n\nAnswer for Task 3\n# regression: outcome variable is Infant_Mortality_Rate\nreg_t3 &lt;- lm(Infant_Mortality_Rate ~ Life_exp_female, data = df)\n\n# Prediction for Kosovo: \n# isolate Kosovo's data :\nKosovo &lt;- df[df$Country_Name == \"Kosovo\", ]\n\n# variables of interest:\nKosovo &lt;- Kosovo |&gt; select(Country_Name, Infant_Mortality_Rate, Life_exp_female)\n# see that Infant Mortality Rate is missing for Kosovo\nView(Kosovo)\n\n# predicted value\nKosovo$predicted_inf_mor &lt;- predict(reg_t3, newdata = Kosovo)\nKosovo$predicted_inf_mor",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html",
    "href": "09_regression_ii.html",
    "title": "9  Regression II",
    "section": "",
    "text": "9.1 Preliminaries\n# Preliminaries ####\n# remove everything from the environment\nrm(list = ls())\n\n# set the working directory:\nsetwd(\"~/methods/\") # adjust the directory address accordingly\n# require the packages \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLet’s import the dataset.\n# Import the dataset: Exam Marks #### \nd &lt;- read.csv(\"data/exam_marks.csv\") # load the data depending on the location on your comouter\nWe have discussed this dataset in the lecture, but let’s quickly remind ourselves of the variables. Here is the codebook.\nTable 9.1: Exam Marks Dataset\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nVariable Explanation\nType of Variable\nPossible Value\n\n\n\n\nexam_id\nID of the exam\nNumerical\n\n\n\nmodule_name\nName of the module\nCategorical\nFrench or Politics\n\n\nmarker_name\nName of the marker\nCategorical\nAlice, Bob, or Charlie\n\n\nattendance_student\nModule attendance rate of the student\nNumerical\n0 – 100%\n\n\ngpa_student\nOverall GPA of the student (previous modules)\nNumerical\n0 – 100%\n\n\nmark\nExam mark\nNumerical\n0 – 100%\nIt is a good idea to use summary() to have a snapshot summary of the variables in the dataset.\n# summary of all variables:\nsummary(d)\n\n    exam_id       module_name        marker_name        attendance_student\n Min.   :240001   Length:100         Length:100         Min.   :  3.00    \n 1st Qu.:240026   Class :character   Class :character   1st Qu.: 26.00    \n Median :240050   Mode  :character   Mode  :character   Median : 45.50    \n Mean   :240050                                         Mean   : 50.60    \n 3rd Qu.:240075                                         3rd Qu.: 73.25    \n Max.   :240100                                         Max.   :100.00    \n  gpa_student         mark      \n Min.   :39.10   Min.   :16.00  \n 1st Qu.:56.45   1st Qu.:46.75  \n Median :63.34   Median :60.00  \n Mean   :64.38   Mean   :61.04  \n 3rd Qu.:73.92   3rd Qu.:75.00  \n Max.   :89.78   Max.   :93.00\nNote that the output displays the numerical summaries of four variables. However, two variables, marker_name and module, are stored as text. These are categorical variables. Let’s see how to deal with them.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html#dealing-with-categorical-variables",
    "href": "09_regression_ii.html#dealing-with-categorical-variables",
    "title": "9  Regression II",
    "section": "9.2 Dealing with categorical variables",
    "text": "9.2 Dealing with categorical variables\nThere are two categorical variables in this dataset: marker_name and module. They are stored as text. We will tell R to consider these as categorical variables, but first, let’s see the categories and respective frequencies.\n\n# frequency table for marker name:\ntable(d$marker_name)\n\n\n  Alice     Bob Charlie \n     32      35      33 \n\n\nThere are three markers Alice, Bob, and Charlie. When you look at the variable, you can see that it is stored as text.\n\n# let's check marker_name\nd$marker_name |&gt; head(5) # show only the first 5 observations of marker\n\n[1] \"Charlie\" \"Alice\"   \"Charlie\" \"Charlie\" \"Bob\"    \n\n\nYou can use factor() to tell R to consider this variable as a categorical variable. Let’s create a new variable marker and store the information in marker_name as a categorical variable.\n\n# generate a new variable called marker: it is a categorical variable\nd$marker &lt;- factor(d$marker_name)\n\nLet’s see how this variable is different from marker_name.\n\n# Display first five observations of marker:\nd$marker |&gt; head(5)\n\n[1] Charlie Alice   Charlie Charlie Bob    \nLevels: Alice Bob Charlie\n\n\nCarefully look at the output above. Observations are no longer enclosed in quotation marks. They are now treated as categories of a categorical variable rather than textual data. Most importantly, the output now displays Levels: and respective categories. The first category, in this case, Alice will be the baseline in a regression analysis.\nYou can also see the difference using class().\n\nclass(d$marker) # class of marker should be factor (meaning categorical)\n\n[1] \"factor\"\n\nclass(d$marker_name) # class of marker_name is character (textual information)\n\n[1] \"character\"\n\n\nYou can also use the summary() function again to see the difference.\n\n# summary of all variables:\nsummary(d)\n\n    exam_id       module_name        marker_name        attendance_student\n Min.   :240001   Length:100         Length:100         Min.   :  3.00    \n 1st Qu.:240026   Class :character   Class :character   1st Qu.: 26.00    \n Median :240050   Mode  :character   Mode  :character   Median : 45.50    \n Mean   :240050                                         Mean   : 50.60    \n 3rd Qu.:240075                                         3rd Qu.: 73.25    \n Max.   :240100                                         Max.   :100.00    \n  gpa_student         mark           marker  \n Min.   :39.10   Min.   :16.00   Alice  :32  \n 1st Qu.:56.45   1st Qu.:46.75   Bob    :35  \n Median :63.34   Median :60.00   Charlie:33  \n Mean   :64.38   Mean   :61.04               \n 3rd Qu.:73.92   3rd Qu.:75.00               \n Max.   :89.78   Max.   :93.00               \n\n\nNotice that the output for marker displays the frequencies of the groups, whereas marker_name just says character. You can also check if respective categories overlap.\n\ntable(d$marker_name, d$marker)\n\n         \n          Alice Bob Charlie\n  Alice      32   0       0\n  Bob         0  35       0\n  Charlie     0   0      33\n\n\nNext, it is time to tell R to store the information in module_name in a categorical variable called module. Try to do it yourself first before seeing the code below.\n\n\nCode\n# module: a factor variable\nd$module &lt;- factor(d$module_name)\n\n\nLet’s see the new variable (and categories). The first category shown in Levels will be the default baseline in any regression analysis.\n\n#  See the categories: Levels tell the categories. \nd$module |&gt; head(5)\n\n[1] Politics Politics Politics Politics French  \nLevels: French Politics\n\n\nIt is a good idea to check if the information recorded is the same.\n\n# table  of : \ntable(d$module_name, d$module)\n\n          \n           French Politics\n  French       50        0\n  Politics      0       50",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html#running-regression-with-a-categorical-variable",
    "href": "09_regression_ii.html#running-regression-with-a-categorical-variable",
    "title": "9  Regression II",
    "section": "9.3 Running regression with a categorical variable",
    "text": "9.3 Running regression with a categorical variable\nLet’s follow the order in the lecture and run a regression of mark on module. The regression equation is below:\n\\[ \\hat{y} = \\alpha + \\beta \\cdot Module \\]\n\n# regression: \nlm(mark ~ module, data = d)\n\n\nCall:\nlm(formula = mark ~ module, data = d)\n\nCoefficients:\n   (Intercept)  modulePolitics  \n         58.54            5.00  \n\n\nRegression output returned two coefficients: (Intercept) and modulePolitics. To use module in the regression analysis, R automatically picked one category as baseline ( French in this case) and turned the other category (Politics) into a binary variable. The regression equation turns into:\n\\[ \\hat{y} = \\alpha + \\beta \\cdot Politics \\]\n\\[ 1. \\text{ When the module is Politics: } Politics =  1 \\]\n\\[ \\hat{y} = \\alpha + \\beta \\cdot 1 \\] \\[ \\hat{y} = \\alpha + \\beta \\]\n\\[ \\hat{y} = 58.54 + 5.00 \\]\n\\[ \\hat{y} = 63.54 \\]\n\\[ 2. \\text{ When the module is French: } Politics = 0 \\]\n\\[ \\hat{y} = \\alpha + \\beta \\cdot 0 \\] \\[ \\hat{y} = \\alpha  \\] \\[ \\hat{y} = 58.54  \\] Recall from the last week that regression returns conditional means in a bivariate analysis with discrete values. Let’s write the respective coefficients and see if this is the case.\n\nd |&gt; group_by(module) |&gt; summarise(module_means = mean(mark))\n\n# A tibble: 2 × 2\n  module   module_means\n  &lt;fct&gt;           &lt;dbl&gt;\n1 French           58.5\n2 Politics         63.5\n\n\n\n9.3.1 Changing the baseline category\nLet’s say, we would like to make French as the baseline and see the coefficient for Politics. We can do this by relevel() function.\n\n# redefine the baseline level (overwrite the existing variable): \nd$module &lt;- relevel(d$module, ref = \"French\")\n\n# run the regression again:\nlm(mark ~ module, data = d)\n\n\nCall:\nlm(formula = mark ~ module, data = d)\n\nCoefficients:\n   (Intercept)  modulePolitics  \n         58.54            5.00  \n\n\n\n\n9.3.2 Task 1: Run a regression with marker\n\nRun a regression of mark on marker. Interpret the results:\n\n\\[ \\hat{y} = \\alpha + \\beta \\cdot Marker \\]\n\n\nShow/Hide the Answer (R Code)\n# run the regression: \nlm(mark ~ marker, data = d)\n\n\n\nMake Bob the baseline category and run the regression again. Interpret the results.\n\n\n\nShow/Hide the Answer (R Code)\n# redefine the baseline level (overwrite the existing variable): \nd$marker &lt;- relevel(d$marker, ref = \"Bob\")\n\n# run the regression again:\nlm(mark ~ marker, data = d)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html#multivariate-regression",
    "href": "09_regression_ii.html#multivariate-regression",
    "title": "9  Regression II",
    "section": "9.4 Multivariate regression",
    "text": "9.4 Multivariate regression\nNext, we will run a series of regression models with multiple controls. Let’s say we are interested in the impact student_gpa on their expected mark. Without any controls, the bivariate regression model is as follows:\n\\[ \\hat{y} = \\alpha + \\beta \\cdot GPA  \\tag{9.1}\\]\n\n# model 1: only gpa\nm1 &lt;- lm(mark ~ gpa_student, data = d)\n\n# see the regression coefficients:\nm1\n\n\nCall:\nlm(formula = mark ~ gpa_student, data = d)\n\nCoefficients:\n(Intercept)  gpa_student  \n    -14.368        1.171  \n\n\n\nAccording to Model 1, one point increase in the student GPA is associated with 1.171 points increase in the expected mark.\n\nHowever, GPA is not the only factor influencing the expected value of the outcome variable. We also know that who the marker is important. Compared to Alice and Charlie, Bob is a more generous marker, giving higher marks on average.\nHow to account for the impact of marker? We could divide the dataset into three by marker and run three regression models. This approach is possible, but it would be long and cumbersome. We will also have three different estimates for the impact of gpa_student, depending on the marker. This is shown in the block below.\n\n\n\n\n\n\nDividing the dataset by marker\n\n\n\n\n\n\n# divide the dataset into three (by marker)\nalice   &lt;- d[d$marker == \"Alice\", ]\nbob     &lt;- d[d$marker == \"Bob\", ]\ncharlie &lt;- d[d$marker == \"Charlie\", ]\n\n# three regression models: \nm_alice   &lt;- lm(mark ~ gpa_student, data = alice)\nm_bob     &lt;- lm(mark ~ gpa_student, data = bob)\nm_charlie &lt;- lm(mark ~ gpa_student, data = charlie)\n\n# see each output:\nm_alice # model for alice\n\n\nCall:\nlm(formula = mark ~ gpa_student, data = alice)\n\nCoefficients:\n(Intercept)  gpa_student  \n    -22.953        1.216  \n\nm_bob # model for bob\n\n\nCall:\nlm(formula = mark ~ gpa_student, data = bob)\n\nCoefficients:\n(Intercept)  gpa_student  \n     -5.090        1.115  \n\nm_charlie # model for charlie\n\n\nCall:\nlm(formula = mark ~ gpa_student, data = charlie)\n\nCoefficients:\n(Intercept)  gpa_student  \n    -14.451        1.166  \n\n\n\nWhen the marker is Alice, one point of increase in a student’s gpa is associated with 1.22 points of increase in their expected exam mark.\nWhen the marker is Bob, one point of increase in a student’s gpa is associated with 1.12 points of increase in their expected exam mark.\nWhen the marker is Charlie, one point of increase in a student’s gpa is associated with 1.17 points of increase in their expected exam mark.\n\n\n\n\nInstead of splitting the dataset into three pieces, a better approach is to consider marker in our regression model directly. In Equation 9.2 below, we are introducing marker as another variable influencing the expected value of our outcome variable, mark.\n\\[ \\hat{y} = \\alpha + \\beta_1 \\cdot GPA +  \\beta_2 \\cdot Marker  \\tag{9.2}\\]\nUsing Equation 9.2, we can have one estimate for the impact of gpa_student while considering the impact of marker. In other words, we are controlling for marker.\nLet’s run Model 2:\n\n# model 2: gpa and marker\nm2 &lt;- lm(mark ~ gpa_student + marker, data = d)\n\n# see the model 2:\nm2\n\n\nCall:\nlm(formula = mark ~ gpa_student + marker, data = d)\n\nCoefficients:\n  (Intercept)    gpa_student    markerAlice  markerCharlie  \n       -8.005          1.160        -11.305         -6.032  \n\n\n\nAccording to Model 2, one point increase in the student GPA is associated with 1.16 points increase in the expected mark, controlling for the marker.\n\nAfter considering who the marker is, there is a tiny change in the expected mark.\n\n9.4.1 Controlling for Attendance Rate\nNext, we will consider the attendance rate of a student. As we have seen in the lecture, there is good reason to expect that gpa_student and attendance_student are correlated, even though a student’s previous marks in different modules have no direct impact on their attendance rate for the current module. However, how hardworking a student is is a factor that influences both gpa_student and attendance_rate, linking the two variables. We can simply refer this underlying influence as student effort. Note that student effort is unobservable. Figure 9.1 is an excerpt from the lecture slides, reflecting our theory.\n\n\n\n\n\n\n\n\n\n\nFigure 9.1: Data generating process according to our theory. The dotted line represents correlation.\n\n\n\nIn Model 3, we will add attendance_student as another predictor variable.\n\\[ \\hat{y} = \\alpha + \\beta_1 \\cdot GPA +  \\beta_2 \\cdot Marker + \\beta_3 \\cdot Attendance   \\tag{9.3}\\]\n\n# Model 3: Adding attendance to the module\nm3 &lt;- lm(mark ~ gpa_student + marker + attendance_student, data = d)\n\n# see the model 3 coefficients:\nm3$coefficients |&gt; round(2)\n\n       (Intercept)        gpa_student        markerAlice      markerCharlie \n             18.62               0.40             -11.56              -7.18 \nattendance_student \n              0.44 \n\n\nLet’s pay close attention to the coefficients from Model 3, particularly to gpa_student and attendance_student.\n\nAccording to Model 3, one point increase in the student GPA is associated with 0.40 points increase in the expected mark, controlling for the marker and the attendance rate.\nAccording to Model 3, one point increase in the attendance rate is associated with 0.44 points increase in the expected mark, controlling for the marker and the student gpa.\n\nLet’s compare Model 3 to Model 2: in Model 2, the estimated impact of gpa_student is almost three times higher than in Model 3. In other words, after controlling for attendance_student, the estimated impact of gpa_student is reduced by more than half.\nIf we do not control for the attendance rate, the GPA also captures the impact of the attendance rate. This is because GPA and attendance rate are correlated.\nIf we run a fourth model with only attendance_student, we will overestimate the impact of the attendance rate because it will capture the impact of the student gpa:\n\\[ \\hat{y} = \\alpha + \\beta \\cdot Attendance \\tag{9.4}\\]\n\n# model 4: \nm4 &lt;- lm(mark ~ attendance_student, data = d)\n\n# see the coefficeints:\nm4$coefficients |&gt; round(2)\n\n       (Intercept) attendance_student \n             32.85               0.56 \n\n\nNote that the estimated impact of attendance_student in Model 4 is increased to 0.56 (from 0.44 in Model 3). If we do not control for the student GPA, the attendance rate also captures the impact of the GPA.\nFor completeness, let’s run Models 5:\n\\[ \\hat{y} = \\alpha + \\beta_1 \\cdot GPA +  \\beta_2 \\cdot Marker + \\beta_3 \\cdot Attendance   + \\beta_4 \\cdot Module \\tag{9.5}\\]\n\n# model 5: \nm5 &lt;- lm(mark ~ gpa_student + marker + attendance_student + module, data = d)\n\n# see the model 5:\nm5\n\n\nCall:\nlm(formula = mark ~ gpa_student + marker + attendance_student + \n    module, data = d)\n\nCoefficients:\n       (Intercept)         gpa_student         markerAlice       markerCharlie  \n           16.9668              0.4175            -11.7748             -7.6233  \nattendance_student      modulePolitics  \n            0.4337              3.0767",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html#regression-table",
    "href": "09_regression_ii.html#regression-table",
    "title": "9  Regression II",
    "section": "9.5 Regression Table",
    "text": "9.5 Regression Table\nAs we have seen in the lecture, there is a standard way to report regression outputs. We will use texreg package to produce high-quality regression tables.\nT\n\n# Install the package (only once):\ninstall.packages(\"texreg\")\n\n\n# call for the package:\nlibrary(texreg)\n\nVersion:  1.39.4\nDate:     2024-07-23\nAuthor:   Philip Leifeld (University of Manchester)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\n\n\nAttaching package: 'texreg'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\n# show a regression table on the screen: for model 1\nscreenreg(m1)\n\n\n=======================\n             Model 1   \n-----------------------\n(Intercept)  -14.37 *  \n              (7.13)   \ngpa_student    1.17 ***\n              (0.11)   \n-----------------------\nR^2            0.54    \nAdj. R^2       0.54    \nNum. obs.    100       \n=======================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n# for multiple models:\nscreenreg(list(m1, m2, m3, m4, m5))\n\n\n==============================================================================\n                    Model 1     Model 2     Model 3     Model 4     Model 5   \n------------------------------------------------------------------------------\n(Intercept)         -14.37 *     -8.00       18.62 ***   32.85 ***   16.97 ***\n                     (7.13)      (7.03)      (4.72)      (1.89)      (4.69)   \ngpa_student           1.17 ***    1.16 ***    0.40 ***                0.42 ***\n                     (0.11)      (0.10)      (0.09)                  (0.08)   \nmarkerAlice                     -11.30 ***  -11.56 ***              -11.77 ***\n                                 (2.84)      (1.72)                  (1.69)   \nmarkerCharlie                    -6.03 *     -7.18 ***               -7.62 ***\n                                 (2.84)      (1.72)                  (1.70)   \nattendance_student                            0.44 ***    0.56 ***    0.43 ***\n                                             (0.03)      (0.03)      (0.03)   \nmodulePolitics                                                        3.08 *  \n                                                                     (1.40)   \n------------------------------------------------------------------------------\nR^2                   0.54        0.61        0.86        0.75        0.86    \nAdj. R^2              0.54        0.59        0.85        0.75        0.86    \nNum. obs.           100         100         100         100         100       \n==============================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n# to change the names of the variables\ntable_var_names &lt;- c(\"Intercept\", \n                     \"Student GPA\", \n                     \"Marker: Alice\", \n                     \"Marker: Charlie\",\n                     \"Attendance Rate\",\n                     \"Module: Politics\"\n                     )\n\n# screen reg:\nscreenreg(list(m1, m2, m3, m4, m5), custom.coef.names = table_var_names)\n\n\n============================================================================\n                  Model 1     Model 2     Model 3     Model 4     Model 5   \n----------------------------------------------------------------------------\nIntercept         -14.37 *     -8.00       18.62 ***   32.85 ***   16.97 ***\n                   (7.13)      (7.03)      (4.72)      (1.89)      (4.69)   \nStudent GPA         1.17 ***    1.16 ***    0.40 ***                0.42 ***\n                   (0.11)      (0.10)      (0.09)                  (0.08)   \nMarker: Alice                 -11.30 ***  -11.56 ***              -11.77 ***\n                               (2.84)      (1.72)                  (1.69)   \nMarker: Charlie                -6.03 *     -7.18 ***               -7.62 ***\n                               (2.84)      (1.72)                  (1.70)   \nAttendance Rate                             0.44 ***    0.56 ***    0.43 ***\n                                           (0.03)      (0.03)      (0.03)   \nModule: Politics                                                    3.08 *  \n                                                                   (1.40)   \n----------------------------------------------------------------------------\nR^2                 0.54        0.61        0.86        0.75        0.86    \nAdj. R^2            0.54        0.59        0.85        0.75        0.86    \nNum. obs.         100         100         100         100         100       \n============================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n# you can also change the Model Names\nscreenreg(list(m_alice, m_bob, m_charlie),\n          custom.model.names = c(\"Alice\", \"Bob\", \"Charlie\")\n          )\n\n\n===============================================\n             Alice       Bob         Charlie   \n-----------------------------------------------\n(Intercept)  -22.95       -5.09      -14.45    \n             (14.02)     (11.12)     (10.50)   \ngpa_student    1.22 ***    1.12 ***    1.17 ***\n              (0.21)      (0.17)      (0.17)   \n-----------------------------------------------\nR^2            0.52        0.58        0.62    \nAdj. R^2       0.50        0.56        0.60    \nNum. obs.     32          35          33       \n===============================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\n\n# to save the regression outout as a word file:\nwordreg(list(m1, m2, m3, m4, m5), \"regression_table.docx\",\n        custom.coef.names = table_var_names)\n\n\n\nprocessing file: filedc95d3819e0.Rmd\n\n\n1/1 [unnamed-chunk-64]\n\n\noutput file: filedc95d3819e0.knit.md\n\n\n/Applications/quarto/bin/tools/pandoc +RTS -K512m -RTS filedc95d3819e0.knit.md --to docx --from markdown+autolink_bare_uris+tex_math_single_backslash --output /Users/baris/Dropbox/teaching_research_methods/website_teaching/regression_table.docx --lua-filter /Library/Frameworks/R.framework/Versions/4.4-arm64/Resources/library/rmarkdown/rmarkdown/lua/pagebreak.lua --highlight-style tango \n\n\n\nOutput created: regression_table.docx",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "09_regression_ii.html#r-squared",
    "href": "09_regression_ii.html#r-squared",
    "title": "9  Regression II",
    "section": "9.6 R-squared",
    "text": "9.6 R-squared\nWe run five regression so far. Each regression model brings slightly (or sometimes largely) different estimates. How should we decide between different models? Your theory should guide your modelling strategy, but sometimes theory is silent or insufficient. In those cases, you can rely on regression metrics.\nR-squared is a measure of how well the regression model accounts for the variation in the outcome variable. It is a measure between 0 (0.00%) and 1 (100.00%).\nIn Model 5, 86% of the variation in mark is explained by the regression model. You can see this by looking at \\(R^2\\) in a regression table.\nR-squared is non-decreasing. It always increases (or stays the same) when you introduce a new variable to the regression model. This is the case whether the new variable makes any sense or not.\nHowever, we would like to be as parsimonious as possible. We want to explain as much as we can with as little as necessary.\nAdjusted R-squared introduces a penalty for each variable added to the model. If a new variable does not bring sufficient improvement, the adjusted R-squared may decrease (or stay the same).\n\n# a nonsensical model\nm_nonsense &lt;- lm(mark ~ gpa_student + exam_id, data = d)\n\nscreenreg(list(m1, m_nonsense), custom.model.names = c(\"Model 1\", \"Nonsense\"))\n\n\n======================================\n             Model 1     Nonsense     \n--------------------------------------\n(Intercept)  -14.37 *      4344.35    \n              (7.13)     (10623.22)   \ngpa_student    1.17 ***       1.16 ***\n              (0.11)         (0.11)   \nexam_id                      -0.02    \n                             (0.04)   \n--------------------------------------\nR^2            0.54           0.54    \nAdj. R^2       0.54           0.53    \nNum. obs.    100            100       \n======================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nCompared to Model 1, the nonsensical model performs worse. This is because exam_id is a roughly random number, having no relationship with exam mark.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>More Regression!</span>"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Introduction to Modern Statistics (2e)",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#introduction-to-modern-statistics-2e",
    "href": "resources.html#introduction-to-modern-statistics-2e",
    "title": "Resources",
    "section": "",
    "text": "Introduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin is an excellent textbook for learning foundational concepts in statistics and data analysis while learning R.\nThe online textbook is free and available at https://openintro-ims.netlify.app/.\nAlso see http://openintro.org/book/ims for supplementary materials and additional resources.",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#hands-on-programming-with-r",
    "href": "resources.html#hands-on-programming-with-r",
    "title": "Resources",
    "section": "Hands-On Programming with R",
    "text": "Hands-On Programming with R\n\n\nHands-on Programming with R by Garrett Grolemund is a straightforward introduction to R. It is useful to learn the basics of R notation.\nWe cover most of the content in Part 1 & 2 in the first four weeks, but if you want to approach the same content from a different angle, you will find this textbook useful.\nIt is freely available here: https://rstudio-education.github.io/hopr/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#r-for-data-science-2e",
    "href": "resources.html#r-for-data-science-2e",
    "title": "Resources",
    "section": "R for Data Science (2e)",
    "text": "R for Data Science (2e)\n\n\n\n\n\nR for Data Science (2e) by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund is an introductory textbook on getting started with R and tidyverse for data management, analysis and visualisation. It is an excellent source to learn the basics of R, R Studio and tidyverse.\nIt can be used as a reference textbook, especially when you are struggling to recall the syntax. It has many examples to get a grasp (or remember) how to use many base R and tidyverse functions.\nIt is free and available here: https://r4ds.hadley.nz/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#r-graphics-cookbook",
    "href": "resources.html#r-graphics-cookbook",
    "title": "Resources",
    "section": "R Graphics Cookbook",
    "text": "R Graphics Cookbook\n\n\nR Graphics Cookbook by Winston Chang is a detailed textbook on creating visualisations in R via using ggplot2.\nIt is free and available here: https://r-graphics.org/",
    "crumbs": [
      "Resources"
    ]
  },
  {
    "objectID": "resources.html#ggplot2-elegant-graphics-for-data-analysis-3e",
    "href": "resources.html#ggplot2-elegant-graphics-for-data-analysis-3e",
    "title": "Resources",
    "section": "ggplot2: Elegant Graphics for Data Analysis (3e)",
    "text": "ggplot2: Elegant Graphics for Data Analysis (3e)\n\n\n\n\n\nThis book explains the underlying theory behind ggplot2. It is available here: https://ggplot2-book.org/",
    "crumbs": [
      "Resources"
    ]
  }
]